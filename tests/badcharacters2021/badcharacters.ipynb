{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "956ba88c",
   "metadata": {},
   "source": [
    "# TextAttack Imperceptible Perturbation Notebook\n",
    "This notebook runs different adversarial attacks using TextAttack.\n",
    "Select the perturbation type below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "249fb75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure experiment\n",
    "perturbation_type = 'homoglyphs'  # options: 'homoglyphs', 'invisible', 'deletions', 'reorderings'\n",
    "store_temp_files = False\n",
    "store_results = False\n",
    "class Args:\n",
    "    pass\n",
    "args = Args()\n",
    "args.perturbation_type = perturbation_type\n",
    "args.store_temp_files = store_temp_files\n",
    "args.store_results = store_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de92dcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from bs4->-r requirements.txt (line 1)) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from beautifulsoup4->bs4->-r requirements.txt (line 1)) (2.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c96ff458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import textattack\n",
    "from textattack.models.wrappers import ModelWrapper\n",
    "from typing import List, Tuple\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "from string import punctuation\n",
    "import argparse\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "import os\n",
    "import tarfile\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from bs4 import BeautifulSoup\n",
    "import torch\n",
    "import importlib.util\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import subprocess\n",
    "import zipfile\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b47de08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "textattack: No entry found for goal function <class 'textattack.goal_functions.custom.targeted_bonus.TargetedBonus'>.\n",
      "textattack: Unknown if model of class <class 'transformers.pipelines.text_classification.TextClassificationPipeline'> compatible with goal function <class 'textattack.goal_functions.custom.targeted_bonus.TargetedBonus'>.\n",
      "textattack: Loading \u001b[94mdatasets\u001b[0m dataset \u001b[94memotion\u001b[0m, split \u001b[94mtest\u001b[0m.\n",
      "textattack: Logging to CSV at path results/emotion/log.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(OrderedDict([('text', 'im feeling rather rotten so im not very ambitious right now')]), 0)\n",
      "Attack(\n",
      "  (search_method): DifferentialEvolution(\n",
      "    (popsize):  32\n",
      "    (maxiter):  10\n",
      "    (max_perturbs):  1\n",
      "    (verbose):  False\n",
      "  )\n",
      "  (goal_function):  TargetedBonus\n",
      "  (transformation):  WordSwapHomoglyphSwap\n",
      "  (constraints): None\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 1 / 0 / 0 / 1:  10%|█         | 1/10 [00:01<00:09,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 1 ---------------------------------------------\n",
      "tensor([9.9889e-01, 2.2335e-04, 2.2042e-04, 2.9064e-04, 1.7530e-04, 1.9613e-04]) --> tensor([9.9899e-01, 1.9190e-04, 1.8884e-04, 2.6047e-04, 1.9274e-04, 1.8098e-04])\n",
      "\n",
      "im feeling rather rotten so im not very [[ambitious]] right now\n",
      "\n",
      "im feeling rather rotten so im not very [[ambitіous]] right now\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 2 / 0 / 0 / 2:  20%|██        | 2/10 [00:01<00:05,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 2 ---------------------------------------------\n",
      "tensor([9.9903e-01, 1.8593e-04, 1.7787e-04, 2.8751e-04, 1.7638e-04, 1.4374e-04]) --> tensor([9.9903e-01, 1.9477e-04, 1.7631e-04, 2.8461e-04, 1.7104e-04, 1.3946e-04])\n",
      "\n",
      "im updating my blog [[because]] i feel shitty\n",
      "\n",
      "im updating my blog [[becauѕe]] i feel shitty\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 3 / 0 / 0 / 3:  30%|███       | 3/10 [00:01<00:04,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 3 ---------------------------------------------\n",
      "tensor([9.9904e-01, 1.8977e-04, 2.0362e-04, 2.2880e-04, 1.8944e-04, 1.4648e-04]) --> tensor([9.9906e-01, 1.7485e-04, 1.8595e-04, 2.4466e-04, 1.9598e-04, 1.3813e-04])\n",
      "\n",
      "i never make her separate from me [[because]] i don t ever want her to feel like i m ashamed with her\n",
      "\n",
      "i never make her separate from me [[bеcause]] i don t ever want her to feel like i m ashamed with her\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 3 / 1 / 0 / 4:  40%|████      | 4/10 [00:02<00:03,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 4 ---------------------------------------------\n",
      "tensor([2.8065e-04, 9.9883e-01, 3.1583e-04, 1.7378e-04, 2.1951e-04, 1.7901e-04]) --> [[[FAILED]]]\n",
      "\n",
      "i left with my bouquet of red and yellow tulips under my arm feeling slightly more optimistic than when i arrived\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 4 / 1 / 0 / 5:  50%|█████     | 5/10 [00:02<00:02,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 5 ---------------------------------------------\n",
      "tensor([9.9895e-01, 2.1511e-04, 1.5600e-04, 2.8785e-04, 2.4783e-04, 1.4445e-04]) --> tensor([9.9899e-01, 2.1313e-04, 1.4978e-04, 2.6780e-04, 2.3862e-04, 1.3681e-04])\n",
      "\n",
      "i was feeling [[a]] little vain when i did this one\n",
      "\n",
      "i was feeling [[а]] little vain when i did this one\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 5 / 1 / 0 / 6:  60%|██████    | 6/10 [00:03<00:02,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 6 ---------------------------------------------\n",
      "tensor([4.3594e-04, 2.3563e-04, 2.2058e-04, 5.7215e-04, 9.9640e-01, 2.1317e-03]) --> tensor([9.9653e-01, 6.6671e-04, 1.2928e-04, 1.0201e-03, 1.4628e-03, 1.9512e-04])\n",
      "\n",
      "i cant walk into a shop anywhere where i do not feel [[uncomfortable]]\n",
      "\n",
      "i cant walk into a shop anywhere where i do not feel [[uncomfortаble]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 5 / 2 / 0 / 7:  70%|███████   | 7/10 [00:03<00:01,  1.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 7 ---------------------------------------------\n",
      "tensor([6.4220e-04, 3.2698e-04, 1.6361e-04, 9.9713e-01, 1.6100e-03, 1.2903e-04]) --> [[[FAILED]]]\n",
      "\n",
      "i felt anger when at the end of a telephone call\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 6 / 2 / 0 / 8:  80%|████████  | 8/10 [00:04<00:01,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 8 ---------------------------------------------\n",
      "tensor([1.4669e-03, 9.8507e-01, 1.1929e-02, 7.0185e-04, 3.9716e-04, 4.3126e-04]) --> tensor([9.5359e-01, 1.8079e-02, 8.0845e-04, 2.3065e-02, 3.9203e-03, 5.3700e-04])\n",
      "\n",
      "i explain why i clung to a relationship with a boy who was in many ways immature and uncommitted despite the excitement i should have been feeling for getting [[accepted]] into the masters program at the university of virginia\n",
      "\n",
      "i explain why i clung to a relationship with a boy who was in many ways immature and uncommitted despite the excitement i should have been feeling for getting [[accepteԁ]] into the masters program at the university of virginia\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 6 / 3 / 0 / 9:  90%|█████████ | 9/10 [00:05<00:00,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 9 ---------------------------------------------\n",
      "tensor([1.7080e-04, 9.9838e-01, 4.2609e-04, 1.5334e-04, 3.0244e-04, 5.6514e-04]) --> [[[FAILED]]]\n",
      "\n",
      "i like to have the same breathless feeling as a reader eager to see what will happen next\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 7 / 3 / 0 / 10: 100%|██████████| 10/10 [00:06<00:00,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 10 ---------------------------------------------\n",
      "tensor([5.1934e-04, 2.9946e-04, 2.3063e-04, 9.9810e-01, 6.6147e-04, 1.9367e-04]) --> tensor([9.9793e-01, 6.0985e-04, 1.4410e-04, 8.7740e-04, 3.1428e-04, 1.2256e-04])\n",
      "\n",
      "i jest i feel [[grumpy]] tired and pre menstrual which i probably am but then again its only been a week and im about as fit as a walrus on vacation for the summer\n",
      "\n",
      "i jest i feel [[grumрy]] tired and pre menstrual which i probably am but then again its only been a week and im about as fit as a walrus on vacation for the summer\n",
      "\n",
      "\n",
      "\n",
      "+-------------------------------+--------+\n",
      "| Attack Results                |        |\n",
      "+-------------------------------+--------+\n",
      "| Number of successful attacks: | 7      |\n",
      "| Number of failed attacks:     | 3      |\n",
      "| Number of skipped attacks:    | 0      |\n",
      "| Original accuracy:            | 100.0% |\n",
      "| Accuracy under attack:        | 30.0%  |\n",
      "| Attack success rate:          | 70.0%  |\n",
      "| Average perturbed word %:     | 6.92%  |\n",
      "| Average num. words per input: | 18.8   |\n",
      "| Avg num queries:              | 207.8  |\n",
      "+-------------------------------+--------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class EmotionWrapper(ModelWrapper):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, input_texts: List[str]) -> List[List[float]]:\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_texts: List[str]\n",
    "\n",
    "        Return:\n",
    "            ret: List[List[float]]\n",
    "            a list of elements, one per element of input_texts. Each element is a list of probabilities, one for each label.\n",
    "        \"\"\"\n",
    "        ret = []\n",
    "        for i in input_texts:\n",
    "            pred = self.model(i)[0]\n",
    "            scores = []\n",
    "            for j in pred:\n",
    "                scores.append(j['score'])\n",
    "            ret.append(scores)\n",
    "        return ret\n",
    "\n",
    "model = pipeline(\"text-classification\", model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True, device=-1)\n",
    "model_wrapper = EmotionWrapper(model)\n",
    "\n",
    "attack = textattack.attack_recipes.BadCharacters2021.build(\n",
    "    model_wrapper, \n",
    "    goal_function_type=\"targeted_bonus\",\n",
    "    perturbation_type=args.perturbation_type\n",
    ")\n",
    "dataset = textattack.datasets.HuggingFaceDataset(\"emotion\", split=\"test\")\n",
    "print(dataset[0])\n",
    "attack_args = textattack.AttackArgs(\n",
    "    num_examples=10,\n",
    "    log_to_csv=\"results/emotion/log.csv\"\n",
    ")\n",
    "attacker = textattack.Attacker(attack, dataset, attack_args)\n",
    "attacker.attack_dataset()\n",
    "\n",
    "if args.store_results == False:\n",
    "    if os.path.isdir(\"results/emotion\"):\n",
    "        shutil.rmtree(\"results/emotion\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ff7dac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "textattack: No entry found for goal function <class 'textattack.goal_functions.custom.named_entity_recognition.NamedEntityRecognition'>.\n",
      "textattack: Unknown if model of class <class 'transformers.pipelines.token_classification.TokenClassificationPipeline'> compatible with goal function <class 'textattack.goal_functions.custom.named_entity_recognition.NamedEntityRecognition'>.\n",
      "textattack: Logging to CSV at path results/named_entity_recognition/log.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): DifferentialEvolution(\n",
      "    (popsize):  32\n",
      "    (maxiter):  10\n",
      "    (max_perturbs):  1\n",
      "    (verbose):  False\n",
      "  )\n",
      "  (goal_function):  NamedEntityRecognition\n",
      "  (transformation):  WordSwapHomoglyphSwap\n",
      "  (constraints): None\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 1 / 0 / 0 / 1:  10%|█         | 1/10 [00:04<00:44,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 1 ---------------------------------------------\n",
      "[\n",
      "  {\n",
      "    \"entity\": \"I-MISC\",\n",
      "    \"score\": 0.9950999617576599,\n",
      "    \"index\": 6,\n",
      "    \"word\": \"J\",\n",
      "    \"start\": 8,\n",
      "    \"end\": 9\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-MISC\",\n",
      "    \"score\": 0.9684495329856873,\n",
      "    \"index\": 7,\n",
      "    \"word\": \"##AP\",\n",
      "    \"start\": 9,\n",
      "    \"end\": 11\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-MISC\",\n",
      "    \"score\": 0.9932603240013123,\n",
      "    \"index\": 8,\n",
      "    \"word\": \"##AN\",\n",
      "    \"start\": 11,\n",
      "    \"end\": 13\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-LOC\",\n",
      "    \"score\": 0.541292130947113,\n",
      "    \"index\": 18,\n",
      "    \"word\": \"CH\",\n",
      "    \"start\": 29,\n",
      "    \"end\": 31\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-LOC\",\n",
      "    \"score\": 0.5053967237472534,\n",
      "    \"index\": 19,\n",
      "    \"word\": \"##IN\",\n",
      "    \"start\": 31,\n",
      "    \"end\": 33\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-ORG\",\n",
      "    \"score\": 0.7646154761314392,\n",
      "    \"index\": 20,\n",
      "    \"word\": \"##A\",\n",
      "    \"start\": 33,\n",
      "    \"end\": 34\n",
      "  }\n",
      "] --> [\n",
      "  {\n",
      "    \"entity\": \"I-MISC\",\n",
      "    \"score\": 0.9942578077316284,\n",
      "    \"index\": 6,\n",
      "    \"word\": \"J\",\n",
      "    \"start\": 8,\n",
      "    \"end\": 9\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-MISC\",\n",
      "    \"score\": 0.9541321992874146,\n",
      "    \"index\": 7,\n",
      "    \"word\": \"##AP\",\n",
      "    \"start\": 9,\n",
      "    \"end\": 11\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-MISC\",\n",
      "    \"score\": 0.9904216527938843,\n",
      "    \"index\": 8,\n",
      "    \"word\": \"##AN\",\n",
      "    \"start\": 11,\n",
      "    \"end\": 13\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.4551286995410919,\n",
      "    \"index\": 11,\n",
      "    \"word\": \"L\",\n",
      "    \"start\": 18,\n",
      "    \"end\": 19\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-LOC\",\n",
      "    \"score\": 0.43001270294189453,\n",
      "    \"index\": 12,\n",
      "    \"word\": \"##U\",\n",
      "    \"start\": 19,\n",
      "    \"end\": 20\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-LOC\",\n",
      "    \"score\": 0.3731754720211029,\n",
      "    \"index\": 13,\n",
      "    \"word\": \"##С\",\n",
      "    \"start\": 20,\n",
      "    \"end\": 21\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.2691863775253296,\n",
      "    \"index\": 14,\n",
      "    \"word\": \"##K\",\n",
      "    \"start\": 21,\n",
      "    \"end\": 22\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-LOC\",\n",
      "    \"score\": 0.5869559049606323,\n",
      "    \"index\": 19,\n",
      "    \"word\": \"CH\",\n",
      "    \"start\": 29,\n",
      "    \"end\": 31\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-LOC\",\n",
      "    \"score\": 0.5285390019416809,\n",
      "    \"index\": 20,\n",
      "    \"word\": \"##IN\",\n",
      "    \"start\": 31,\n",
      "    \"end\": 33\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-ORG\",\n",
      "    \"score\": 0.7942052483558655,\n",
      "    \"index\": 21,\n",
      "    \"word\": \"##A\",\n",
      "    \"start\": 33,\n",
      "    \"end\": 34\n",
      "  }\n",
      "]\n",
      "\n",
      "SOCCER- JAPAN GET [[LUCKY]] WIN, CHINA IN SURPRISE DEFEAT.\n",
      "\n",
      "SOCCER- JAPAN GET [[LUСKY]] WIN, CHINA IN SURPRISE DEFEAT.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 2 / 0 / 0 / 2:  20%|██        | 2/10 [00:05<00:23,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 2 ---------------------------------------------\n",
      "[\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.9996979236602783,\n",
      "    \"index\": 1,\n",
      "    \"word\": \"Na\",\n",
      "    \"start\": 0,\n",
      "    \"end\": 2\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.9929589033126831,\n",
      "    \"index\": 2,\n",
      "    \"word\": \"##di\",\n",
      "    \"start\": 2,\n",
      "    \"end\": 4\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.9990008473396301,\n",
      "    \"index\": 3,\n",
      "    \"word\": \"##m\",\n",
      "    \"start\": 4,\n",
      "    \"end\": 5\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.9996190071105957,\n",
      "    \"index\": 4,\n",
      "    \"word\": \"La\",\n",
      "    \"start\": 6,\n",
      "    \"end\": 8\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.9780004620552063,\n",
      "    \"index\": 5,\n",
      "    \"word\": \"##d\",\n",
      "    \"start\": 8,\n",
      "    \"end\": 9\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.9701477885246277,\n",
      "    \"index\": 6,\n",
      "    \"word\": \"##ki\",\n",
      "    \"start\": 9,\n",
      "    \"end\": 11\n",
      "  }\n",
      "] --> [\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.9996484518051147,\n",
      "    \"index\": 1,\n",
      "    \"word\": \"Na\",\n",
      "    \"start\": 0,\n",
      "    \"end\": 2\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.9934110045433044,\n",
      "    \"index\": 2,\n",
      "    \"word\": \"##di\",\n",
      "    \"start\": 2,\n",
      "    \"end\": 4\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.9991694688796997,\n",
      "    \"index\": 3,\n",
      "    \"word\": \"##m\",\n",
      "    \"start\": 4,\n",
      "    \"end\": 5\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.9991093277931213,\n",
      "    \"index\": 4,\n",
      "    \"word\": \"L\",\n",
      "    \"start\": 6,\n",
      "    \"end\": 7\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.9616693258285522,\n",
      "    \"index\": 5,\n",
      "    \"word\": \"##а\",\n",
      "    \"start\": 7,\n",
      "    \"end\": 8\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.8536984324455261,\n",
      "    \"index\": 6,\n",
      "    \"word\": \"##d\",\n",
      "    \"start\": 8,\n",
      "    \"end\": 9\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.9816961288452148,\n",
      "    \"index\": 7,\n",
      "    \"word\": \"##ki\",\n",
      "    \"start\": 9,\n",
      "    \"end\": 11\n",
      "  }\n",
      "]\n",
      "\n",
      "Nadim [[Ladki]]\n",
      "\n",
      "Nadim [[Lаdki]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 2 / 1 / 0 / 3:  30%|███       | 3/10 [00:07<00:16,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 3 ---------------------------------------------\n",
      "[\n",
      "  {\n",
      "    \"entity\": \"I-LOC\",\n",
      "    \"score\": 0.9984487295150757,\n",
      "    \"index\": 1,\n",
      "    \"word\": \"AL\",\n",
      "    \"start\": 0,\n",
      "    \"end\": 2\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-LOC\",\n",
      "    \"score\": 0.9677120447158813,\n",
      "    \"index\": 2,\n",
      "    \"word\": \"-\",\n",
      "    \"start\": 2,\n",
      "    \"end\": 3\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-LOC\",\n",
      "    \"score\": 0.995570957660675,\n",
      "    \"index\": 3,\n",
      "    \"word\": \"AI\",\n",
      "    \"start\": 3,\n",
      "    \"end\": 5\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-LOC\",\n",
      "    \"score\": 0.9939817786216736,\n",
      "    \"index\": 4,\n",
      "    \"word\": \"##N\",\n",
      "    \"start\": 5,\n",
      "    \"end\": 6\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-LOC\",\n",
      "    \"score\": 0.9997562766075134,\n",
      "    \"index\": 6,\n",
      "    \"word\": \"United\",\n",
      "    \"start\": 8,\n",
      "    \"end\": 14\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-LOC\",\n",
      "    \"score\": 0.9997227787971497,\n",
      "    \"index\": 7,\n",
      "    \"word\": \"Arab\",\n",
      "    \"start\": 15,\n",
      "    \"end\": 19\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-LOC\",\n",
      "    \"score\": 0.9998668432235718,\n",
      "    \"index\": 8,\n",
      "    \"word\": \"Emirates\",\n",
      "    \"start\": 20,\n",
      "    \"end\": 28\n",
      "  }\n",
      "] --> [[[FAILED]]]\n",
      "\n",
      "AL-AIN, United Arab Emirates 1996-12-06\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 2 / 2 / 0 / 4:  40%|████      | 4/10 [00:10<00:15,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 4 ---------------------------------------------\n",
      "[\n",
      "  {\n",
      "    \"entity\": \"I-LOC\",\n",
      "    \"score\": 0.9998020529747009,\n",
      "    \"index\": 1,\n",
      "    \"word\": \"Japan\",\n",
      "    \"start\": 0,\n",
      "    \"end\": 5\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-MISC\",\n",
      "    \"score\": 0.9762685298919678,\n",
      "    \"index\": 7,\n",
      "    \"word\": \"Asian\",\n",
      "    \"start\": 33,\n",
      "    \"end\": 38\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-MISC\",\n",
      "    \"score\": 0.9974727034568787,\n",
      "    \"index\": 8,\n",
      "    \"word\": \"Cup\",\n",
      "    \"start\": 39,\n",
      "    \"end\": 42\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-LOC\",\n",
      "    \"score\": 0.9997666478157043,\n",
      "    \"index\": 18,\n",
      "    \"word\": \"Syria\",\n",
      "    \"start\": 78,\n",
      "    \"end\": 83\n",
      "  }\n",
      "] --> [[[FAILED]]]\n",
      "\n",
      "Japan began the defence of their Asian Cup title with a lucky 2-1 win against Syria in a Group C championship match on Friday.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 2 / 3 / 0 / 5:  50%|█████     | 5/10 [00:14<00:14,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 5 ---------------------------------------------\n",
      "[\n",
      "  {\n",
      "    \"entity\": \"I-LOC\",\n",
      "    \"score\": 0.9998183846473694,\n",
      "    \"index\": 2,\n",
      "    \"word\": \"China\",\n",
      "    \"start\": 4,\n",
      "    \"end\": 9\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-LOC\",\n",
      "    \"score\": 0.9996594190597534,\n",
      "    \"index\": 27,\n",
      "    \"word\": \"Uzbekistan\",\n",
      "    \"start\": 118,\n",
      "    \"end\": 128\n",
      "  }\n",
      "] --> [[[FAILED]]]\n",
      "\n",
      "But China saw their luck desert them in the second match of the group, crashing to a surprise 2-0 defeat to newcomers Uzbekistan.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 3 / 3 / 0 / 6:  60%|██████    | 6/10 [00:22<00:14,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 6 ---------------------------------------------\n",
      "[\n",
      "  {\n",
      "    \"entity\": \"I-LOC\",\n",
      "    \"score\": 0.9997816681861877,\n",
      "    \"index\": 1,\n",
      "    \"word\": \"China\",\n",
      "    \"start\": 0,\n",
      "    \"end\": 5\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-MISC\",\n",
      "    \"score\": 0.9979750514030457,\n",
      "    \"index\": 18,\n",
      "    \"word\": \"U\",\n",
      "    \"start\": 93,\n",
      "    \"end\": 94\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-MISC\",\n",
      "    \"score\": 0.989479124546051,\n",
      "    \"index\": 19,\n",
      "    \"word\": \"##z\",\n",
      "    \"start\": 94,\n",
      "    \"end\": 95\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-MISC\",\n",
      "    \"score\": 0.996921956539154,\n",
      "    \"index\": 20,\n",
      "    \"word\": \"##bek\",\n",
      "    \"start\": 95,\n",
      "    \"end\": 98\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.9994372725486755,\n",
      "    \"index\": 22,\n",
      "    \"word\": \"Igor\",\n",
      "    \"start\": 107,\n",
      "    \"end\": 111\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.9995008707046509,\n",
      "    \"index\": 23,\n",
      "    \"word\": \"S\",\n",
      "    \"start\": 112,\n",
      "    \"end\": 113\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.9525274038314819,\n",
      "    \"index\": 24,\n",
      "    \"word\": \"##h\",\n",
      "    \"start\": 113,\n",
      "    \"end\": 114\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.7246343493461609,\n",
      "    \"index\": 25,\n",
      "    \"word\": \"##k\",\n",
      "    \"start\": 114,\n",
      "    \"end\": 115\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.8198036551475525,\n",
      "    \"index\": 26,\n",
      "    \"word\": \"##vy\",\n",
      "    \"start\": 115,\n",
      "    \"end\": 117\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.9968863129615784,\n",
      "    \"index\": 27,\n",
      "    \"word\": \"##rin\",\n",
      "    \"start\": 117,\n",
      "    \"end\": 120\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-MISC\",\n",
      "    \"score\": 0.999038815498352,\n",
      "    \"index\": 47,\n",
      "    \"word\": \"Chinese\",\n",
      "    \"start\": 205,\n",
      "    \"end\": 212\n",
      "  }\n",
      "] --> [\n",
      "  {\n",
      "    \"entity\": \"I-LOC\",\n",
      "    \"score\": 0.9997778534889221,\n",
      "    \"index\": 1,\n",
      "    \"word\": \"China\",\n",
      "    \"start\": 0,\n",
      "    \"end\": 5\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-MISC\",\n",
      "    \"score\": 0.9980767965316772,\n",
      "    \"index\": 18,\n",
      "    \"word\": \"U\",\n",
      "    \"start\": 93,\n",
      "    \"end\": 94\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-MISC\",\n",
      "    \"score\": 0.989856481552124,\n",
      "    \"index\": 19,\n",
      "    \"word\": \"##z\",\n",
      "    \"start\": 94,\n",
      "    \"end\": 95\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-MISC\",\n",
      "    \"score\": 0.9967934489250183,\n",
      "    \"index\": 20,\n",
      "    \"word\": \"##bek\",\n",
      "    \"start\": 95,\n",
      "    \"end\": 98\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.9990713596343994,\n",
      "    \"index\": 22,\n",
      "    \"word\": \"I\",\n",
      "    \"start\": 107,\n",
      "    \"end\": 108\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.9601768255233765,\n",
      "    \"index\": 23,\n",
      "    \"word\": \"##g\",\n",
      "    \"start\": 108,\n",
      "    \"end\": 109\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.9561796188354492,\n",
      "    \"index\": 24,\n",
      "    \"word\": \"##ο\",\n",
      "    \"start\": 109,\n",
      "    \"end\": 110\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.9983696341514587,\n",
      "    \"index\": 25,\n",
      "    \"word\": \"##r\",\n",
      "    \"start\": 110,\n",
      "    \"end\": 111\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.9997169375419617,\n",
      "    \"index\": 26,\n",
      "    \"word\": \"S\",\n",
      "    \"start\": 112,\n",
      "    \"end\": 113\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.958065390586853,\n",
      "    \"index\": 27,\n",
      "    \"word\": \"##h\",\n",
      "    \"start\": 113,\n",
      "    \"end\": 114\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.7516828179359436,\n",
      "    \"index\": 28,\n",
      "    \"word\": \"##k\",\n",
      "    \"start\": 114,\n",
      "    \"end\": 115\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.8039079904556274,\n",
      "    \"index\": 29,\n",
      "    \"word\": \"##vy\",\n",
      "    \"start\": 115,\n",
      "    \"end\": 117\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.9888207912445068,\n",
      "    \"index\": 30,\n",
      "    \"word\": \"##rin\",\n",
      "    \"start\": 117,\n",
      "    \"end\": 120\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-MISC\",\n",
      "    \"score\": 0.9990665316581726,\n",
      "    \"index\": 50,\n",
      "    \"word\": \"Chinese\",\n",
      "    \"start\": 205,\n",
      "    \"end\": 212\n",
      "  }\n",
      "]\n",
      "\n",
      "China controlled most of the match and saw several chances missed until the 78th minute when Uzbek striker [[Igor]] Shkvyrin took advantage of a misdirected defensive header to lob the ball over the advancing Chinese keeper and into an empty net.\n",
      "\n",
      "China controlled most of the match and saw several chances missed until the 78th minute when Uzbek striker [[Igοr]] Shkvyrin took advantage of a misdirected defensive header to lob the ball over the advancing Chinese keeper and into an empty net.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 4 / 3 / 0 / 7:  70%|███████   | 7/10 [00:25<00:11,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 7 ---------------------------------------------\n",
      "[\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.9992066025733948,\n",
      "    \"index\": 1,\n",
      "    \"word\": \"Ole\",\n",
      "    \"start\": 0,\n",
      "    \"end\": 3\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.9995224475860596,\n",
      "    \"index\": 2,\n",
      "    \"word\": \"##g\",\n",
      "    \"start\": 3,\n",
      "    \"end\": 4\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.9996681213378906,\n",
      "    \"index\": 3,\n",
      "    \"word\": \"S\",\n",
      "    \"start\": 5,\n",
      "    \"end\": 6\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.9951527118682861,\n",
      "    \"index\": 4,\n",
      "    \"word\": \"##hat\",\n",
      "    \"start\": 6,\n",
      "    \"end\": 9\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.9679039716720581,\n",
      "    \"index\": 5,\n",
      "    \"word\": \"##ski\",\n",
      "    \"start\": 9,\n",
      "    \"end\": 12\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.9797117114067078,\n",
      "    \"index\": 6,\n",
      "    \"word\": \"##ku\",\n",
      "    \"start\": 12,\n",
      "    \"end\": 14\n",
      "  }\n",
      "] --> [\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.9989537000656128,\n",
      "    \"index\": 1,\n",
      "    \"word\": \"Ole\",\n",
      "    \"start\": 0,\n",
      "    \"end\": 3\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.9995146989822388,\n",
      "    \"index\": 2,\n",
      "    \"word\": \"##g\",\n",
      "    \"start\": 3,\n",
      "    \"end\": 4\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.9996217489242554,\n",
      "    \"index\": 3,\n",
      "    \"word\": \"S\",\n",
      "    \"start\": 5,\n",
      "    \"end\": 6\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.9259416460990906,\n",
      "    \"index\": 4,\n",
      "    \"word\": \"##h\",\n",
      "    \"start\": 6,\n",
      "    \"end\": 7\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.9733062386512756,\n",
      "    \"index\": 5,\n",
      "    \"word\": \"##а\",\n",
      "    \"start\": 7,\n",
      "    \"end\": 8\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.9973963499069214,\n",
      "    \"index\": 6,\n",
      "    \"word\": \"##tsk\",\n",
      "    \"start\": 8,\n",
      "    \"end\": 11\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.8436657190322876,\n",
      "    \"index\": 7,\n",
      "    \"word\": \"##ik\",\n",
      "    \"start\": 11,\n",
      "    \"end\": 13\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.9229859113693237,\n",
      "    \"index\": 8,\n",
      "    \"word\": \"##u\",\n",
      "    \"start\": 13,\n",
      "    \"end\": 14\n",
      "  }\n",
      "]\n",
      "\n",
      "Oleg [[Shatskiku]] made sure of the win in injury time, hitting an unstoppable left foot shot from just outside the area.\n",
      "\n",
      "Oleg [[Shаtskiku]] made sure of the win in injury time, hitting an unstoppable left foot shot from just outside the area.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 4 / 4 / 0 / 8:  80%|████████  | 8/10 [00:28<00:07,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 8 ---------------------------------------------\n",
      "[\n",
      "  {\n",
      "    \"entity\": \"I-MISC\",\n",
      "    \"score\": 0.9985392093658447,\n",
      "    \"index\": 3,\n",
      "    \"word\": \"Soviet\",\n",
      "    \"start\": 11,\n",
      "    \"end\": 17\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-MISC\",\n",
      "    \"score\": 0.9799859523773193,\n",
      "    \"index\": 9,\n",
      "    \"word\": \"Asian\",\n",
      "    \"start\": 45,\n",
      "    \"end\": 50\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-MISC\",\n",
      "    \"score\": 0.9974244832992554,\n",
      "    \"index\": 10,\n",
      "    \"word\": \"Cup\",\n",
      "    \"start\": 51,\n",
      "    \"end\": 54\n",
      "  }\n",
      "] --> [[[FAILED]]]\n",
      "\n",
      "The former Soviet republic was playing in an Asian Cup finals tie for the first time.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 5 / 4 / 0 / 9:  90%|█████████ | 9/10 [00:32<00:03,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 9 ---------------------------------------------\n",
      "[\n",
      "  {\n",
      "    \"entity\": \"I-MISC\",\n",
      "    \"score\": 0.9735670685768127,\n",
      "    \"index\": 4,\n",
      "    \"word\": \"Asian\",\n",
      "    \"start\": 20,\n",
      "    \"end\": 25\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-MISC\",\n",
      "    \"score\": 0.9971873164176941,\n",
      "    \"index\": 5,\n",
      "    \"word\": \"Games\",\n",
      "    \"start\": 26,\n",
      "    \"end\": 31\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-LOC\",\n",
      "    \"score\": 0.9997908473014832,\n",
      "    \"index\": 11,\n",
      "    \"word\": \"Uzbekistan\",\n",
      "    \"start\": 53,\n",
      "    \"end\": 63\n",
      "  }\n",
      "] --> [\n",
      "  {\n",
      "    \"entity\": \"I-PER\",\n",
      "    \"score\": 0.637923002243042,\n",
      "    \"index\": 1,\n",
      "    \"word\": \"D\",\n",
      "    \"start\": 0,\n",
      "    \"end\": 1\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-ORG\",\n",
      "    \"score\": 0.7454715967178345,\n",
      "    \"index\": 2,\n",
      "    \"word\": \"##е\",\n",
      "    \"start\": 1,\n",
      "    \"end\": 2\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-ORG\",\n",
      "    \"score\": 0.8040305972099304,\n",
      "    \"index\": 3,\n",
      "    \"word\": \"##sp\",\n",
      "    \"start\": 2,\n",
      "    \"end\": 4\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-ORG\",\n",
      "    \"score\": 0.8762766122817993,\n",
      "    \"index\": 4,\n",
      "    \"word\": \"##ite\",\n",
      "    \"start\": 4,\n",
      "    \"end\": 7\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-MISC\",\n",
      "    \"score\": 0.9774352312088013,\n",
      "    \"index\": 7,\n",
      "    \"word\": \"Asian\",\n",
      "    \"start\": 20,\n",
      "    \"end\": 25\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-MISC\",\n",
      "    \"score\": 0.996993899345398,\n",
      "    \"index\": 8,\n",
      "    \"word\": \"Games\",\n",
      "    \"start\": 26,\n",
      "    \"end\": 31\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-LOC\",\n",
      "    \"score\": 0.9997883439064026,\n",
      "    \"index\": 14,\n",
      "    \"word\": \"Uzbekistan\",\n",
      "    \"start\": 53,\n",
      "    \"end\": 63\n",
      "  }\n",
      "]\n",
      "\n",
      "[[Despite]] winning the Asian Games title two years ago, Uzbekistan are in the finals as outsiders.\n",
      "\n",
      "[[Dеspite]] winning the Asian Games title two years ago, Uzbekistan are in the finals as outsiders.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 5 / 5 / 0 / 10: 100%|██████████| 10/10 [00:36<00:00,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 10 ---------------------------------------------\n",
      "[\n",
      "  {\n",
      "    \"entity\": \"I-LOC\",\n",
      "    \"score\": 0.9997974038124084,\n",
      "    \"index\": 12,\n",
      "    \"word\": \"Japan\",\n",
      "    \"start\": 64,\n",
      "    \"end\": 69\n",
      "  },\n",
      "  {\n",
      "    \"entity\": \"I-LOC\",\n",
      "    \"score\": 0.9998229146003723,\n",
      "    \"index\": 27,\n",
      "    \"word\": \"Syria\",\n",
      "    \"start\": 154,\n",
      "    \"end\": 159\n",
      "  }\n",
      "] --> [[[FAILED]]]\n",
      "\n",
      "Two goals from defensive errors in the last six minutes allowed Japan to come from behind and collect all three points from their opening meeting against Syria.\n",
      "\n",
      "\n",
      "\n",
      "+-------------------------------+--------+\n",
      "| Attack Results                |        |\n",
      "+-------------------------------+--------+\n",
      "| Number of successful attacks: | 5      |\n",
      "| Number of failed attacks:     | 5      |\n",
      "| Number of skipped attacks:    | 0      |\n",
      "| Original accuracy:            | 100.0% |\n",
      "| Accuracy under attack:        | 50.0%  |\n",
      "| Attack success rate:          | 50.0%  |\n",
      "| Average perturbed word %:     | 14.91% |\n",
      "| Average num. words per input: | 18.4   |\n",
      "| Avg num queries:              | 191.8  |\n",
      "+-------------------------------+--------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class NERModelWrapper(ModelWrapper):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, input_texts: List[str]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_texts: List[str]\n",
    "        \n",
    "        Return:\n",
    "            ret\n",
    "                Model output\n",
    "        \"\"\"\n",
    "        ret = []\n",
    "        for i in input_texts:\n",
    "            pred = self.model(i)\n",
    "            ret.append(pred)\n",
    "        return ret\n",
    "\n",
    "model = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "model_wrapper = NERModelWrapper(model)\n",
    "ner_classes = ['PER', 'ORG', 'LOC', 'MISC']\n",
    "attack = textattack.attack_recipes.BadCharacters2021.build(\n",
    "    model_wrapper, \n",
    "    goal_function_type=\"named_entity_recognition\", \n",
    "    perturbation_type=args.perturbation_type, \n",
    "    target_suffix=ner_classes[0]\n",
    ")\n",
    "dataset = load_dataset(\"conll2003\", split=\"test\", trust_remote_code=True)\n",
    "pairs = []\n",
    "def detokenize(tokens: List[str]) -> str:\n",
    "    output = \"\"\n",
    "    for index, token in enumerate(tokens):\n",
    "        if (len(token) == 1 and token in punctuation) or index == 0:\n",
    "            output += token\n",
    "        else:\n",
    "            output += ' ' + token\n",
    "    return output\n",
    "for ex in dataset:\n",
    "    tokens = ex[\"tokens\"]\n",
    "    ner_labels = ex[\"ner_tags\"]\n",
    "    text = detokenize(tokens) \n",
    "    pairs.append((text, \"NER\")) # hack\n",
    "dataset = textattack.datasets.Dataset(pairs)\n",
    "attack_args = textattack.AttackArgs(\n",
    "    num_examples=10,\n",
    "    log_to_csv=\"results/named_entity_recognition/log.csv\"\n",
    ")\n",
    "attacker = textattack.Attacker(attack, dataset, attack_args)\n",
    "attacker.attack_dataset()\n",
    "if args.store_results == False:\n",
    "    if os.path.isdir(\"results/named_entity_recognition\"):\n",
    "        shutil.rmtree(\"results/named_entity_recognition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "421e6616",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vlwk/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:340: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(f, map_location=torch.device(\"cpu\"))\n",
      "/opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "/opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages/hydra/experimental/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n",
      "  deprecation_warning(message=message)\n",
      "/opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages/hydra/core/default_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/Users/vlwk/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n",
      "/opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages/hydra/compose.py:56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/upgrades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n",
      "/opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "/Users/vlwk/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/fairseq_model.py:272: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading WMT14 test data from http://statmt.org/wmt14/test-full.tgz...\n",
      "Extracting test-full/newstest2014-fren-ref.fr.sgm to temp/translation/data\n",
      "Extracting test-full/newstest2014-fren-src.en.sgm to temp/translation/data\n",
      "en_fr dataset downloaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: No entry found for goal function <class 'textattack.goal_functions.text.maximize_levenshtein.MaximizeLevenshtein'>.\n",
      "textattack: Unknown if model of class <class 'fairseq.hub_utils.GeneratorHubInterface'> compatible with goal function <class 'textattack.goal_functions.text.maximize_levenshtein.MaximizeLevenshtein'>.\n",
      "textattack: Logging to CSV at path results/translation/log.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(OrderedDict([('text', 'Spectacular Wingsuit Jump Over Bogota')]), 'Spectaculaire saut en \"wingsuit\" au-dessus de Bogota')\n",
      "Attack(\n",
      "  (search_method): DifferentialEvolution(\n",
      "    (popsize):  32\n",
      "    (maxiter):  10\n",
      "    (max_perturbs):  1\n",
      "    (verbose):  False\n",
      "  )\n",
      "  (goal_function):  MaximizeLevenshtein(\n",
      "    (maximizable):  False\n",
      "    (target_distance):  0.1\n",
      "  )\n",
      "  (transformation):  WordSwapHomoglyphSwap\n",
      "  (constraints): None\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 1 / 0 / 0 / 1: 100%|██████████| 1/1 [00:11<00:00, 11.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 1 ---------------------------------------------\n",
      "Spectaculaire combinaison pour les ailes sauter au-dessus de Bogota --> Spectaculaire saut Wingѕuit au-dessus de Bogota\n",
      "\n",
      "Spectacular [[Wingsuit]] Jump Over Bogota\n",
      "\n",
      "Spectacular [[Wingѕuit]] Jump Over Bogota\n",
      "\n",
      "\n",
      "\n",
      "+-------------------------------+--------+\n",
      "| Attack Results                |        |\n",
      "+-------------------------------+--------+\n",
      "| Number of successful attacks: | 1      |\n",
      "| Number of failed attacks:     | 0      |\n",
      "| Number of skipped attacks:    | 0      |\n",
      "| Original accuracy:            | 100.0% |\n",
      "| Accuracy under attack:        | 0.0%   |\n",
      "| Attack success rate:          | 100.0% |\n",
      "| Average perturbed word %:     | 20.0%  |\n",
      "| Average num. words per input: | 5.0    |\n",
      "| Avg num queries:              | 291.0  |\n",
      "+-------------------------------+--------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class FairseqTranslationWrapper(ModelWrapper):\n",
    "    \"\"\"\n",
    "    A wrapper for the model\n",
    "        torch.hub.load('pytorch/fairseq',\n",
    "                        'transformer.wmt14.en-fr',\n",
    "                        tokenizer='moses',\n",
    "                        bpe='subword_nmt',\n",
    "                        verbose=False).eval()\n",
    "    or any other model with a .translate() method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model  \n",
    "\n",
    "    def __call__(self, text_input_list: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_texts: List[str]\n",
    "        \n",
    "        Return:\n",
    "            ret: List[str]\n",
    "                Result of translation. One per element in input_texts.\n",
    "        \"\"\"\n",
    "        return [self.model.translate(text) for text in text_input_list]\n",
    "model = torch.hub.load(\n",
    "    'pytorch/fairseq',\n",
    "    'transformer.wmt14.en-fr',\n",
    "    tokenizer='moses',\n",
    "    bpe='subword_nmt',\n",
    "    verbose=False\n",
    ").eval()\n",
    "\n",
    "model_wrapper = FairseqTranslationWrapper(model)\n",
    "def download_en_fr_dataset():\n",
    "    \n",
    "\n",
    "    # Define constants\n",
    "    url = \"http://statmt.org/wmt14/test-full.tgz\"\n",
    "    target_dir = os.path.join(\"temp/translation\", \"data\")\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Downloading WMT14 test data from {url}...\")\n",
    "\n",
    "    # Download and extract in-memory\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    with tarfile.open(fileobj=BytesIO(response.content), mode=\"r:gz\") as tar:\n",
    "        for member in tar.getmembers():\n",
    "            if member.name.endswith(\"newstest2014-fren-src.en.sgm\") or member.name.endswith(\"newstest2014-fren-ref.fr.sgm\"):\n",
    "                print(f\"Extracting {member.name} to {target_dir}\")\n",
    "                member.name = os.path.basename(member.name) \n",
    "                tar.extract(member, path=target_dir)\n",
    "\n",
    "    print(\"en_fr dataset downloaded.\")\n",
    "def load_en_fr_dataset():\n",
    "    \"\"\"\n",
    "    Loads English-French sentence pairs from SGM files and returns a TextAttack Dataset.\n",
    "\n",
    "    Returns:\n",
    "        textattack.datasets.Dataset: wrapped dataset of (English, French) pairs.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    source_path = os.path.join(\"temp/translation\", \"data/newstest2014-fren-src.en.sgm\")\n",
    "    target_path = os.path.join(\"temp/translation\", \"data/newstest2014-fren-ref.fr.sgm\")\n",
    "\n",
    "    with open(source_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        source_doc = BeautifulSoup(f, \"html.parser\")\n",
    "\n",
    "    with open(target_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        target_doc = BeautifulSoup(f, \"html.parser\")\n",
    "\n",
    "    pairs = []\n",
    "\n",
    "    for doc in source_doc.find_all(\"doc\"):\n",
    "        docid = str(doc[\"docid\"])\n",
    "        for seg in doc.find_all(\"seg\"):\n",
    "            segid = str(seg[\"id\"])\n",
    "            src = str(seg.string).strip() if seg.string else \"\"\n",
    "            tgt_node = target_doc.select_one(f'doc[docid=\"{docid}\"] > seg[id=\"{segid}\"]')\n",
    "            if tgt_node and tgt_node.string:\n",
    "                tgt = str(tgt_node.string).strip()\n",
    "                pairs.append((src, tgt))\n",
    "    return textattack.datasets.Dataset(pairs) \n",
    "download_en_fr_dataset()\n",
    "dataset = load_en_fr_dataset()\n",
    "attack = textattack.attack_recipes.BadCharacters2021.build(\n",
    "    model_wrapper, \n",
    "    goal_function_type=\"maximize_levenshtein\", \n",
    "    perturbation_type=args.perturbation_type,\n",
    "    target_distance=0.1\n",
    ")\n",
    "print(dataset[0])\n",
    "attack_args = textattack.AttackArgs(\n",
    "    num_examples=1,\n",
    "    log_to_csv=\"results/translation/log.csv\"\n",
    ")\n",
    "attacker = textattack.Attacker(attack, dataset, attack_args)\n",
    "attacker.attack_dataset()\n",
    "\n",
    "if args.store_temp_files == False:\n",
    "    if os.path.isdir(\"temp/translation\"):\n",
    "        shutil.rmtree(\"temp/translation\")\n",
    "if args.store_results == False:\n",
    "    if os.path.isdir(\"results/translation\"):\n",
    "        shutil.rmtree(\"results/translation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18c57faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://codait-cos-max.s3.us.cloud-object-storage.appdomain.cloud/max-toxic-comment-classifier/1.0.0/assets.tar.gz\n",
      "Extracting assets...\n",
      "Running: git clone https://github.com/IBM/MAX-Toxic-Comment-Classifier.git\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'MAX-Toxic-Comment-Classifier'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: pip install -r temp/toxic/toxic/requirements.txt\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from -r temp/toxic/toxic/requirements.txt (line 1)) (2.5.1)\n",
      "Requirement already satisfied: pytorch-pretrained-bert in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from -r temp/toxic/toxic/requirements.txt (line 2)) (0.6.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from torch->-r temp/toxic/toxic/requirements.txt (line 1)) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from torch->-r temp/toxic/toxic/requirements.txt (line 1)) (4.13.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from torch->-r temp/toxic/toxic/requirements.txt (line 1)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from torch->-r temp/toxic/toxic/requirements.txt (line 1)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from torch->-r temp/toxic/toxic/requirements.txt (line 1)) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from torch->-r temp/toxic/toxic/requirements.txt (line 1)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from sympy==1.13.1->torch->-r temp/toxic/toxic/requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from pytorch-pretrained-bert->-r temp/toxic/toxic/requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: boto3 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from pytorch-pretrained-bert->-r temp/toxic/toxic/requirements.txt (line 2)) (1.35.63)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from pytorch-pretrained-bert->-r temp/toxic/toxic/requirements.txt (line 2)) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from pytorch-pretrained-bert->-r temp/toxic/toxic/requirements.txt (line 2)) (4.67.0)\n",
      "Requirement already satisfied: regex in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from pytorch-pretrained-bert->-r temp/toxic/toxic/requirements.txt (line 2)) (2024.11.6)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.63 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from boto3->pytorch-pretrained-bert->-r temp/toxic/toxic/requirements.txt (line 2)) (1.35.63)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from boto3->pytorch-pretrained-bert->-r temp/toxic/toxic/requirements.txt (line 2)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from boto3->pytorch-pretrained-bert->-r temp/toxic/toxic/requirements.txt (line 2)) (0.10.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from jinja2->torch->-r temp/toxic/toxic/requirements.txt (line 1)) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from requests->pytorch-pretrained-bert->-r temp/toxic/toxic/requirements.txt (line 2)) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from requests->pytorch-pretrained-bert->-r temp/toxic/toxic/requirements.txt (line 2)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from requests->pytorch-pretrained-bert->-r temp/toxic/toxic/requirements.txt (line 2)) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from requests->pytorch-pretrained-bert->-r temp/toxic/toxic/requirements.txt (line 2)) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from botocore<1.36.0,>=1.35.63->boto3->pytorch-pretrained-bert->-r temp/toxic/toxic/requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.63->boto3->pytorch-pretrained-bert->-r temp/toxic/toxic/requirements.txt (line 2)) (1.16.0)\n",
      "Running: pip install maxfw\n",
      "Requirement already satisfied: maxfw in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (1.1.6)\n",
      "Requirement already satisfied: flask-restx>=0.3 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from maxfw) (1.3.0)\n",
      "Requirement already satisfied: flask-cors>=3.0.9 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from maxfw) (5.0.0)\n",
      "Requirement already satisfied: Pillow>=8.1.1 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from maxfw) (11.0.0)\n",
      "Requirement already satisfied: numpy>=1.18.4 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from maxfw) (1.26.4)\n",
      "Requirement already satisfied: Flask>=0.9 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from flask-cors>=3.0.9->maxfw) (3.1.0)\n",
      "Requirement already satisfied: aniso8601>=0.82 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from flask-restx>=0.3->maxfw) (10.0.0)\n",
      "Requirement already satisfied: jsonschema in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from flask-restx>=0.3->maxfw) (4.23.0)\n",
      "Requirement already satisfied: werkzeug!=2.0.0 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from flask-restx>=0.3->maxfw) (3.1.3)\n",
      "Requirement already satisfied: pytz in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from flask-restx>=0.3->maxfw) (2024.2)\n",
      "Requirement already satisfied: importlib-resources in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from flask-restx>=0.3->maxfw) (6.4.5)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from Flask>=0.9->flask-cors>=3.0.9->maxfw) (3.1.4)\n",
      "Requirement already satisfied: itsdangerous>=2.2 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from Flask>=0.9->flask-cors>=3.0.9->maxfw) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from Flask>=0.9->flask-cors>=3.0.9->maxfw) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.9 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from Flask>=0.9->flask-cors>=3.0.9->maxfw) (1.9.0)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from Flask>=0.9->flask-cors>=3.0.9->maxfw) (7.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from werkzeug!=2.0.0->flask-restx>=0.3->maxfw) (3.0.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from importlib-resources->flask-restx>=0.3->maxfw) (3.20.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from jsonschema->flask-restx>=0.3->maxfw) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from jsonschema->flask-restx>=0.3->maxfw) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from jsonschema->flask-restx>=0.3->maxfw) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from jsonschema->flask-restx>=0.3->maxfw) (0.22.3)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages (from referencing>=0.28.4->jsonschema->flask-restx>=0.3->maxfw) (4.13.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vlwk/Documents/GitHub/TextAttack/tests/badcharacters2021/temp/toxic/toxic/core/model.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(DEFAULT_MODEL_PATH+'pytorch_model.bin', map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading temp/toxic/toxicity_annotated_comments.tsv\n",
      "Downloading temp/toxic/toxicity_annotations.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: No entry found for goal function <class 'textattack.goal_functions.custom.logit_sum.LogitSum'>.\n",
      "textattack: Unknown if model of class <class 'temp.toxic.toxic.core.model.ModelWrapper'> compatible with goal function <class 'textattack.goal_functions.custom.logit_sum.LogitSum'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(OrderedDict([('text', \"`  After the wasted bit on his sexuality, I haven't read the above diatribe but one thing for ABSOLUTE certain: Never in the history of this planet has Umberto ever been referred to as Humbert. It is this kind of lunacy that renders Wikipedia as a ``fringe`` place for idiots to play. Get serious....DW`\")]), 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Logging to CSV at path results/toxic/log.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): DifferentialEvolution(\n",
      "    (popsize):  32\n",
      "    (maxiter):  10\n",
      "    (max_perturbs):  1\n",
      "    (verbose):  False\n",
      "  )\n",
      "  (goal_function):  LogitSum\n",
      "  (transformation):  WordSwapHomoglyphSwap\n",
      "  (constraints): None\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 1 / 0 / 1:  10%|█         | 1/10 [00:05<00:52,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 1 ---------------------------------------------\n",
      "tensor([9.6299e-01, 1.7065e-03, 6.9568e-02, 8.5043e-04, 6.6601e-01, 1.0821e-02]) --> [[[FAILED]]]\n",
      "\n",
      "`  After the wasted bit on his sexuality, I haven't read the above diatribe but one thing for ABSOLUTE certain: Never in the history of this planet has Umberto ever been referred to as Humbert. It is this kind of lunacy that renders Wikipedia as a ``fringe`` place for idiots to play. Get serious....DW`\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 2 / 0 / 2:  20%|██        | 2/10 [00:23<01:35, 11.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 2 ---------------------------------------------\n",
      "tensor([9.5041e-01, 1.1281e-03, 1.2786e-01, 2.0196e-03, 6.5274e-03, 4.9106e-04]) --> [[[FAILED]]]\n",
      "\n",
      "    Erik, for crying out loud. You legally can have sex with children, but not with underage children. Underage children are children beneath the legal age of consent (different states have 14, 15, 16, 17 or 18 as the underage limit. Children in most countries means someone under the age of 18. In Ireland for example, a 'child' aged 17 is over the age of consent. In Britain it is 16. Do you know the first thing about the topic?   The name is standard english asage. Scandal is a POV term. Allegation allows the discussion of cases that have not yet come to court, which cannot for technical reasons be prosecuted and cases that were thrown up as invalid. Roman Catholic Church sex abuses allegations in standard english means allegations about the Roman Catholic Church.   Public relations is minor tabloid point when you are discussing the rape of children, so stop adding in an irrelevant topic.   Your mucking around lost a lot of changes, spelling corrections and other problems solved. I was caught in an edit conflict with you and given the scale of the changes could do nothing but a cut and paste job. I then tried to go in to your version to transfer over accurate changes but then you started this insane fucking around with the article's location, using a POV title. I will keep moving back to a carefully worded neutral title.  \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 3 / 0 / 3:  30%|███       | 3/10 [00:28<01:06,  9.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 3 ---------------------------------------------\n",
      "tensor([0.9890, 0.0210, 0.6732, 0.0114, 0.0362, 0.0010]) --> [[[FAILED]]]\n",
      "\n",
      "BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS, BOOBS\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 1 / 3 / 0 / 4:  40%|████      | 4/10 [00:30<00:45,  7.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 4 ---------------------------------------------\n",
      "tensor([6.3730e-01, 2.2119e-04, 6.5575e-03, 2.2104e-04, 6.4976e-03, 1.8520e-04]) --> tensor([7.4618e-04, 1.0445e-04, 2.3478e-04, 8.5751e-05, 1.3375e-04, 9.1187e-05])\n",
      "\n",
      "  It was anonymous on accident, and FYI, I'm not a troll [[moron]].  \n",
      "\n",
      "  It was anonymous on accident, and FYI, I'm not a troll [[mοron]].  \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 2 / 3 / 0 / 5:  50%|█████     | 5/10 [00:31<00:31,  6.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 5 ---------------------------------------------\n",
      "tensor([9.5362e-01, 5.9087e-04, 2.3544e-03, 2.6504e-03, 6.0207e-02, 8.6768e-04]) --> tensor([2.4677e-03, 9.4472e-05, 2.4309e-04, 1.1691e-04, 1.6267e-04, 1.6336e-04])\n",
      "\n",
      "a dippy [[lunatic]] like \n",
      "\n",
      "a dippy [[lunаtic]] like \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 2 / 4 / 0 / 6:  60%|██████    | 6/10 [00:34<00:22,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 6 ---------------------------------------------\n",
      "tensor([0.9787, 0.0508, 0.1007, 0.1988, 0.2891, 0.9054]) --> [[[FAILED]]]\n",
      "\n",
      "Yes, complain to your rabbi and then go shoot some more defenseless Arabs to take out your frustration.    \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 3 / 4 / 0 / 7:  70%|███████   | 7/10 [00:35<00:15,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 7 ---------------------------------------------\n",
      "tensor([9.8629e-01, 3.5898e-02, 9.6445e-01, 7.1178e-04, 8.3451e-01, 3.1259e-03]) --> tensor([5.9527e-04, 8.8887e-05, 1.6067e-04, 7.6018e-05, 1.3248e-04, 9.9863e-05])\n",
      "\n",
      "  i am using the sandbox, [[ass]] wipe\n",
      "\n",
      "  i am using the sandbox, [[aѕs]] wipe\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 3 / 5 / 0 / 8:  80%|████████  | 8/10 [00:37<00:09,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 8 ---------------------------------------------\n",
      "tensor([0.9889, 0.0698, 0.9102, 0.0015, 0.9791, 0.0200]) --> [[[FAILED]]]\n",
      "\n",
      "  == whoa ==  you are a bi fat idiot, stop spamming my user space.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 3 / 6 / 0 / 9:  90%|█████████ | 9/10 [00:41<00:04,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 9 ---------------------------------------------\n",
      "tensor([0.9972, 0.5847, 0.9971, 0.0080, 0.9786, 0.0395]) --> [[[FAILED]]]\n",
      "\n",
      "  == FUCK YOU THUE ==  YOU BRAIN DEAD FUCK, YOU CAN'T BLOCK ME. THANKS TO PROXY SERVERS AND ANON BROWSING I CAN COME TO WIKI ZILLION TIMES SO IF SCUM LIKE YOU THINK THAT YOU CAN BLOCK ME THEN YOU ARE IN FOR BIG SURPRISE YOU FAGGOT. I AM GIVING YOU LAST CHANCE IN CIVILIZED WAY TO STOP VANDALIZING KASH JAFFREY. DON'T FUCK WITH CANADIAN CONTENT, ITS TO ADVANCE FOR YOUR EMPTY BRAIN TO CONCIEVE. AND LASTLY IF YOU WANT TO LEAVE MESSAGE THEN WRITE (DON'T USE PREDEFINE ONE LINERS.) SEE YA ASSHOLE  JOEY\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 4 / 6 / 0 / 10: 100%|██████████| 10/10 [00:48<00:00,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 10 ---------------------------------------------\n",
      "tensor([7.6723e-01, 1.3083e-03, 2.5661e-01, 8.6072e-04, 2.6488e-03, 3.2799e-04]) --> tensor([4.4767e-04, 8.7316e-05, 1.3527e-04, 7.5941e-05, 1.3710e-04, 1.1336e-04])\n",
      "\n",
      "`  :: Hey, the only persons ``suffering`` were those who read that [[bullshit]], Wikipedia opening itself up to a Copyright infringement suit, and the credibility of those who are facilitating both.  The length of the protections will not influence my decision to revert the article back to a previous version that does not contain plagiarism.  `\n",
      "\n",
      "`  :: Hey, the only persons ``suffering`` were those who read that [[bullѕhit]], Wikipedia opening itself up to a Copyright infringement suit, and the credibility of those who are facilitating both.  The length of the protections will not influence my decision to revert the article back to a previous version that does not contain plagiarism.  `\n",
      "\n",
      "\n",
      "\n",
      "+-------------------------------+--------+\n",
      "| Attack Results                |        |\n",
      "+-------------------------------+--------+\n",
      "| Number of successful attacks: | 4      |\n",
      "| Number of failed attacks:     | 6      |\n",
      "| Number of skipped attacks:    | 0      |\n",
      "| Original accuracy:            | 100.0% |\n",
      "| Accuracy under attack:        | 60.0%  |\n",
      "| Attack success rate:          | 40.0%  |\n",
      "| Average perturbed word %:     | 12.36% |\n",
      "| Average num. words per input: | 85.3   |\n",
      "| Avg num queries:              | 252.6  |\n",
      "+-------------------------------+--------+"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class IBMMAXToxicWrapper(ModelWrapper):\n",
    "    \"\"\"\n",
    "    A wrapper for the IBM Max Toxic model\n",
    "    https://github.com/IBM/MAX-Toxic-Comment-Classifier/blob/master/core/model.py\n",
    "    \"\"\"\n",
    "    def __init__(self, ibm_model_wrapper):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ibm_model_wrapper: An instance of the IBM MAX Toxic `ModelWrapper()` class.\n",
    "        \"\"\"\n",
    "        self.model = ibm_model_wrapper\n",
    "\n",
    "    def __call__(self, input_text_list: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_texts: List[str]\n",
    "        \n",
    "        Return:\n",
    "            ret: np.ndarray\n",
    "                One entry per element in input_text_list. Each is a list of logits, one for each label.\n",
    "        \"\"\"\n",
    "        self.model._pre_process(input_text_list)\n",
    "        logits = self.model._predict(input_text_list)\n",
    "        return np.array(logits)\n",
    "\n",
    "# Download model\n",
    "\n",
    "def run(cmd):\n",
    "    print(f\"Running: {cmd}\")\n",
    "    subprocess.run(cmd, shell=True, check=True)\n",
    "os.makedirs(\"temp/toxic\", exist_ok=True)\n",
    "shutil.rmtree(os.path.join(\"temp/toxic\", \"assets\"), ignore_errors=True)\n",
    "shutil.rmtree(os.path.join(\"temp/toxic\", \"toxic\"), ignore_errors=True)\n",
    "os.makedirs(os.path.join(\"temp/toxic\", \"assets\"), exist_ok=True)\n",
    "url = \"https://codait-cos-max.s3.us.cloud-object-storage.appdomain.cloud/max-toxic-comment-classifier/1.0.0/assets.tar.gz\"\n",
    "tar_path = os.path.join(\"temp/toxic\", \"assets/assets.tar.gz\")\n",
    "print(f\"Downloading {url}\")\n",
    "response = requests.get(url)\n",
    "with open(tar_path, \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "print(\"Extracting assets...\")\n",
    "with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "    tar.extractall(\"temp/toxic/assets\")\n",
    "os.remove(tar_path)\n",
    "run(\"git clone https://github.com/IBM/MAX-Toxic-Comment-Classifier.git\")\n",
    "shutil.move(\"MAX-Toxic-Comment-Classifier\", os.path.join(\"temp/toxic\", \"toxic\"))\n",
    "requirements_path = os.path.join(\"temp/toxic\", \"toxic/requirements.txt\")\n",
    "with open(requirements_path, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "with open(requirements_path, \"w\") as f:\n",
    "    for line in lines:\n",
    "        f.write(line.split(\"==\")[0].strip() + \"\\n\")\n",
    "run(f\"pip install -r {requirements_path}\")\n",
    "run(\"pip install maxfw\")\n",
    "model_py_path = os.path.join(\"temp/toxic\", \"toxic/core/model.py\")\n",
    "with open(model_py_path, \"r\") as f:\n",
    "    content = f.read()\n",
    "content = content.replace(\"from config\", \"from ..config\")\n",
    "content = content.replace(\"from core.\", \"from .\")\n",
    "with open(model_py_path, \"w\") as f:\n",
    "    f.write(content)\n",
    "with open(\"temp/toxic/toxic/config.py\", \"r\") as f:\n",
    "    content = f.read()\n",
    "content = content.replace(\"assets\", \"temp/toxic/assets\")\n",
    "with open(\"temp/toxic/toxic/config.py\", \"w\") as f:\n",
    "    f.write(content)\n",
    "importlib.invalidate_caches()\n",
    "\n",
    "def load_model_wrapper():\n",
    "    module_name = \"temp.toxic.toxic.core.model\"\n",
    "    \n",
    "    # Remove the module if it's already loaded\n",
    "    if module_name in sys.modules:\n",
    "        importlib.reload(sys.modules[module_name])\n",
    "    else:\n",
    "        importlib.import_module(module_name)\n",
    "    \n",
    "    module = sys.modules[module_name]\n",
    "    return module.ModelWrapper()\n",
    "\n",
    "model = load_model_wrapper()\n",
    "model_wrapper = IBMMAXToxicWrapper(model)\n",
    "\n",
    "# Get data\n",
    "if (os.path.exists(\"temp/toxic/toxicity_annotated_comments.tsv\") == False) or (os.path.exists(\"temp/toxic/toxicity_annotations.tsv\") == False): \n",
    "    data_urls = {\n",
    "        \"temp/toxic/toxicity_annotated_comments.tsv\": \"https://ndownloader.figshare.com/files/7394542\",\n",
    "        \"temp/toxic/toxicity_annotations.tsv\": \"https://ndownloader.figshare.com/files/7394539\",\n",
    "    }\n",
    "    for filename, url in data_urls.items():\n",
    "        print(f\"Downloading {filename}\")\n",
    "        response = requests.get(url)\n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "comments = pd.read_csv('temp/toxic/toxicity_annotated_comments.tsv', sep = '\\t', index_col = 0)\n",
    "annotations = pd.read_csv('temp/toxic/toxicity_annotations.tsv',  sep = '\\t')\n",
    "# labels a comment as toxic if the majority of annoatators did so\n",
    "labels = annotations.groupby('rev_id')['toxicity'].mean() > 0.5\n",
    "# join labels and comments\n",
    "comments['toxicity'] = labels\n",
    "# remove newline and tab tokens\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\n",
    "test_comments = comments.query(\"split=='test'\").query(\"toxicity==True\")\n",
    "examples = test_comments.reset_index().to_dict('records')\n",
    "\n",
    "pairs = [(row[\"comment\"], 0) for row in examples]\n",
    "\n",
    "dataset = textattack.datasets.Dataset(pairs)\n",
    "print(dataset[0])\n",
    "attack = textattack.attack_recipes.BadCharacters2021.build(\n",
    "    model_wrapper, \n",
    "    goal_function_type=\"logit_sum\", \n",
    "    perturbation_type=args.perturbation_type\n",
    ")\n",
    "attack_args = textattack.AttackArgs(\n",
    "    num_examples=10,\n",
    "    log_to_csv=\"results/toxic/log.csv\"\n",
    ")\n",
    "attacker = textattack.Attacker(attack, dataset, attack_args)\n",
    "attacker.attack_dataset()\n",
    "if args.store_temp_files == False:\n",
    "    if os.path.isdir(\"temp/toxic\"):\n",
    "        shutil.rmtree(\"temp/toxic\")\n",
    "if args.store_results == False:\n",
    "    if os.path.isdir(\"results/toxic\"):\n",
    "        shutil.rmtree(\"results/toxic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b14d474d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/vlwk/.cache/torch/hub/pytorch_fairseq_main\n",
      "/Users/vlwk/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:340: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(f, map_location=torch.device(\"cpu\"))\n",
      "/opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages/hydra/experimental/initialize.py:43: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
      "  deprecation_warning(message=message)\n",
      "/opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages/hydra/experimental/initialize.py:45: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  self.delegate = real_initialize(\n",
      "/opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages/hydra/experimental/compose.py:25: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n",
      "  deprecation_warning(message=message)\n",
      "/opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages/hydra/core/default_element.py:124: UserWarning: In 'config': Usage of deprecated keyword in package header '# @package _group_'.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
      "  deprecation_warning(\n",
      "/Users/vlwk/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:450: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n",
      "/opt/anaconda3/envs/projenvconda39/lib/python3.9/site-packages/hydra/compose.py:56: UserWarning: \n",
      "The strict flag in the compose API is deprecated.\n",
      "See https://hydra.cc/docs/1.2/upgrades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
      "\n",
      "  deprecation_warning(\n",
      "/Users/vlwk/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:369: UserWarning: \n",
      "'config' is validated against ConfigStore schema with the same name.\n",
      "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
      "  x = hub_utils.from_pretrained(\n",
      "textattack: No entry found for goal function <class 'textattack.goal_functions.custom.targeted_strict.TargetedStrict'>.\n",
      "textattack: Unknown if model of class <class 'fairseq.models.roberta.hub_interface.RobertaHubInterface'> compatible with goal function <class 'textattack.goal_functions.custom.targeted_strict.TargetedStrict'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(OrderedDict([('premise', 'The new rights are nice enough'), ('hypothesis', 'Everyone really likes the newest benefits ')]), 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Logging to CSV at path results/mnli/log.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack(\n",
      "  (search_method): DifferentialEvolution(\n",
      "    (popsize):  32\n",
      "    (maxiter):  10\n",
      "    (max_perturbs):  1\n",
      "    (verbose):  False\n",
      "  )\n",
      "  (goal_function):  TargetedStrict\n",
      "  (transformation):  WordSwapHomoglyphSwap\n",
      "  (constraints): None\n",
      "  (is_black_box):  True\n",
      ") \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 1 / 0 / 1:  10%|█         | 1/10 [00:03<00:30,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 1 ---------------------------------------------\n",
      "tensor([0.0271, 0.9637, 0.0092]) --> [[[FAILED]]]\n",
      "\n",
      "[[[[Premise]]]]: The new rights are nice enough\n",
      "[[[[Hypothesis]]]]: Everyone really likes the newest benefits \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 0 / 2 / 0 / 2:  20%|██        | 2/10 [00:08<00:35,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 2 ---------------------------------------------\n",
      "tensor([9.9939e-01, 4.5619e-04, 1.5498e-04]) --> [[[FAILED]]]\n",
      "\n",
      "[[[[Premise]]]]: This site includes a list of all award winners and a searchable database of Government Executive articles.\n",
      "[[[[Hypothesis]]]]: The Government Executive articles housed on the website are not able to be searched.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 1 / 2 / 0 / 3:  30%|███       | 3/10 [00:13<00:31,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 3 ---------------------------------------------\n",
      "tensor([7.4583e-04, 1.2614e-02, 9.8664e-01]) --> tensor([0.5641, 0.1481, 0.2878])\n",
      "\n",
      "[[[[Premise]]]]: uh i don't know i i have mixed emotions about him uh sometimes i [[like]] him but at the same times i love to see somebody beat him\n",
      "[[[[Hypothesis]]]]: I like him for the most part, but would still enjoy seeing someone beat him.\n",
      "\n",
      "uh i don't know i i have mixed emotions about him uh sometimes i [[lіke]] him but at the same times i love to see somebody beat him\n",
      "I like him for the most part, but would still enjoy seeing someone beat him.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 1 / 3 / 0 / 4:  40%|████      | 4/10 [00:18<00:27,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 4 ---------------------------------------------\n",
      "tensor([9.9908e-01, 7.1604e-04, 2.0551e-04]) --> [[[FAILED]]]\n",
      "\n",
      "[[[[Premise]]]]: yeah i i think my favorite restaurant is always been the one closest  you know the closest as long as it's it meets the minimum criteria you know of good food\n",
      "[[[[Hypothesis]]]]: My favorite restaurants are always at least a hundred miles away from my house. \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 1 / 4 / 0 / 5:  50%|█████     | 5/10 [00:20<00:20,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 5 ---------------------------------------------\n",
      "tensor([9.9951e-01, 3.0989e-04, 1.8339e-04]) --> [[[FAILED]]]\n",
      "\n",
      "[[[[Premise]]]]: i don't know um do you do a lot of camping\n",
      "[[[[Hypothesis]]]]: I know exactly.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 2 / 4 / 0 / 6:  60%|██████    | 6/10 [00:24<00:16,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 6 ---------------------------------------------\n",
      "tensor([9.9896e-01, 6.2569e-04, 4.1762e-04]) --> tensor([0.4893, 0.1376, 0.3731])\n",
      "\n",
      "[[[[Premise]]]]: well that would be a help i [[wish]] they would do that here we have got so little landfill space left that we're going to run out before the end of this decade and it's really going to be\n",
      "[[[[Hypothesis]]]]: We have plenty of space in the landfill.\n",
      "\n",
      "well that would be a help i [[wіsh]] they would do that here we have got so little landfill space left that we're going to run out before the end of this decade and it's really going to be\n",
      "We have plenty of space in the landfill.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 2 / 5 / 0 / 7:  70%|███████   | 7/10 [00:28<00:12,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 7 ---------------------------------------------\n",
      "tensor([9.9934e-01, 4.8667e-04, 1.7737e-04]) --> [[[FAILED]]]\n",
      "\n",
      "[[[[Premise]]]]: yeah i know and i did that all through college and it worked too\n",
      "[[[[Hypothesis]]]]: I did that all through college but it never worked \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 3 / 5 / 0 / 8:  80%|████████  | 8/10 [00:33<00:08,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 8 ---------------------------------------------\n",
      "tensor([0.0014, 0.5608, 0.4378]) --> tensor([0.6773, 0.1157, 0.2070])\n",
      "\n",
      "[[[[Premise]]]]: Calcutta seems to be the [[only]] other production center having any pretensions to artistic creativity at all, but ironically you're actually more likely to see the works of Satyajit Ray or Mrinal Sen shown in Europe or North America than in India itself.\n",
      "[[[[Hypothesis]]]]: Most of Mrinal Sen's work can be found in European collections.\n",
      "\n",
      "Calcutta seems to be the [[onlу]] other production center having any pretensions to artistic creativity at all, but ironically you're actually more likely to see the works of Satyajit Ray or Mrinal Sen shown in Europe or North America than in India itself.\n",
      "Most of Mrinal Sen's work can be found in European collections.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 4 / 5 / 0 / 9:  90%|█████████ | 9/10 [00:37<00:04,  4.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 9 ---------------------------------------------\n",
      "tensor([0.8050, 0.1832, 0.0118]) --> tensor([0.6584, 0.1275, 0.2141])\n",
      "\n",
      "[[[[Premise]]]]: If that investor were willing to pay extra for the security of limited downside, she could buy put options with a strike price of $98, which would lock [[in]] her profit on the shares at $18, less whatever the options cost.\n",
      "[[[[Hypothesis]]]]: THe strike price could be $8.\n",
      "\n",
      "If that investor were willing to pay extra for the security of limited downside, she could buy put options with a strike price of $98, which would lock [[іn]] her profit on the shares at $18, less whatever the options cost.\n",
      "THe strike price could be $8.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Succeeded / Failed / Skipped / Total] 4 / 6 / 0 / 10: 100%|██████████| 10/10 [00:42<00:00,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------- Result 10 ---------------------------------------------\n",
      "tensor([0.0309, 0.9665, 0.0026]) --> [[[FAILED]]]\n",
      "\n",
      "[[[[Premise]]]]: 3)  Dare you rise to the occasion, like Raskolnikov, and reject the petty rules that govern lesser men?\n",
      "[[[[Hypothesis]]]]: Would you rise up and defeaat all evil lords in the town?\n",
      "\n",
      "\n",
      "\n",
      "+-------------------------------+--------+\n",
      "| Attack Results                |        |\n",
      "+-------------------------------+--------+\n",
      "| Number of successful attacks: | 4      |\n",
      "| Number of failed attacks:     | 6      |\n",
      "| Number of skipped attacks:    | 0      |\n",
      "| Original accuracy:            | 100.0% |\n",
      "| Accuracy under attack:        | 60.0%  |\n",
      "| Attack success rate:          | 40.0%  |\n",
      "| Average perturbed word %:     | 2.11%  |\n",
      "| Average num. words per input: | 34.7   |\n",
      "| Avg num queries:              | 99.0   |\n",
      "+-------------------------------+--------+"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class FairseqMnliWrapper(ModelWrapper):\n",
    "    \"\"\"\n",
    "    A wrapper for the model\n",
    "        torch.hub.load('pytorch/fairseq', 'roberta.large.mnli').eval()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, input_texts: List[Tuple[str, str]]) -> List[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_texts: List[Tuple[str, str]]\n",
    "                List of (premise, hypothesis)\n",
    "        \n",
    "        Return:\n",
    "            ret: List[torch.Tensor]\n",
    "                Each tensor is a list of probabilities, \n",
    "                one for each of (contradiction, neutral, entailment)\n",
    "        \"\"\"\n",
    "        ret = []\n",
    "        for t in input_texts:\n",
    "            premise = t[0]\n",
    "            hypothesis = t[1]\n",
    "            tokens = self.model.encode(premise, hypothesis)\n",
    "            predict = self.model.predict('mnli', tokens)\n",
    "            probs = softmax(predict, dim=1).cpu().detach()[0]\n",
    "            ret.append(probs.unsqueeze(0))\n",
    "        return ret\n",
    "model = torch.hub.load('pytorch/fairseq',\n",
    "    'roberta.large.mnli').eval()\n",
    "model_wrapper = FairseqMnliWrapper(model)\n",
    "url = \"https://cims.nyu.edu/~sbowman/multinli/multinli_1.0.zip\"\n",
    "os.makedirs(\"temp/mnli\", exist_ok=True)  \n",
    "response = requests.get(url)\n",
    "response.raise_for_status() \n",
    "with zipfile.ZipFile(BytesIO(response.content)) as zip_ref:\n",
    "    zip_ref.extractall(\"temp/mnli\")\n",
    "label_map = {'contradiction': 0, 'neutral': 1, 'entailment': 2}\n",
    "pairs = []\n",
    "with open(\"temp/mnli/multinli_1.0/multinli_1.0_dev_matched.jsonl\", 'r') as f:\n",
    "    for line in f:\n",
    "        sample = json.loads(line)\n",
    "        if sample['gold_label'] not in label_map:\n",
    "            continue\n",
    "\n",
    "        premise = sample['sentence1']\n",
    "        hypothesis = sample['sentence2']\n",
    "        label = label_map[sample['gold_label']]\n",
    "        item = ((premise, hypothesis), label)\n",
    "        pairs.append(item)\n",
    "dataset = textattack.datasets.Dataset(pairs, input_columns=[\"premise\", \"hypothesis\"])\n",
    "print(dataset[0])\n",
    "attack = textattack.attack_recipes.BadCharacters2021.build(\n",
    "    model_wrapper, \n",
    "    goal_function_type=\"targeted_strict\", \n",
    "    perturbation_type=args.perturbation_type\n",
    ")\n",
    "attack_args = textattack.AttackArgs(\n",
    "    num_examples=10,\n",
    "    log_to_csv=\"results/mnli/log.csv\"\n",
    ")\n",
    "attacker = textattack.Attacker(attack, dataset, attack_args)\n",
    "attacker.attack_dataset()\n",
    "if args.store_temp_files == False:\n",
    "    if os.path.isdir(\"temp/mnli\"):\n",
    "        shutil.rmtree(\"temp/mnli\")\n",
    "if args.store_results == False:\n",
    "    if os.path.isdir(\"results/mnli\"):\n",
    "        shutil.rmtree(\"results/mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6e99244",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.store_temp_files == False:\n",
    "    if os.path.isdir(\"temp\"):\n",
    "        shutil.rmtree(\"temp\")\n",
    "\n",
    "if args.store_results == False:\n",
    "    if os.path.isdir(\"results\"):\n",
    "        shutil.rmtree(\"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5de720",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projenvconda39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

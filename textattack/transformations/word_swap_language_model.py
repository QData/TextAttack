import torch

from textattack.shared import utils
from textattack.transformations.word_swap import WordSwap
from textattack.tokenizers import BERTTokenizer
from transformers.modeling_bert import BertForMaskedLM
from transformers import GPT2Tokenizer, GPT2LMHeadModel

class WordSwapLanguageModel(WordSwap):
    """
        Transforms an input by replacing a word with the most likely words generated by a language model.
        Currently, only supports BERT
    """

    BERT_PATH = 'models/bert_masked_lm'
    GPT2_PATH = 'models/gpt2_lm'

    def __init__(self, max_candidates=100, language_model="bert",
        replace_stopwords=False, **kwargs):
        super().__init__(**kwargs)
        self.max_candidates = max_candidates
        self.language_model = language_model
        self.replace_stopwords = replace_stopwords

        if language_model == "bert":
            self.tokenizer = BERTTokenizer()
            #model_file_path = utils.download_if_needed(BERT_PATH)
            self.model = BertForMaskedLM.from_pretrained('bert-base-uncased')

        elif language_model == 'gpt-2':
            self.tokenizer = GPT2Tokenizer.from_pretrained('gpt-2')
            self.model = GPT2LMHeadModel.from_pretrained('gpt2')

        self.model.to(utils.get_device())
        self.model.eval()

    def __call__(self, tokenized_text, indices_to_replace=None):
        """
        Returns a list of possible 'candidate words' to replace a word in a sentence
            or phrase based off top predictions by a language model
        """
        text = tokenized_text.words
        if not indices_to_replace:
            indices_to_replace = list(range(len(text)))

        tokens_list = []
        segments_list = []
        masked_indices = []

        for i in indices_to_replace:

            if not self.replace_stopwords and text[i].lower() in self.stopwords:
                continue
            masked_indices.append(i)
            masked_text = list(text)
            masked_text[i] = "[MASK]"
            masked_text = " ".join(masked_text)
            tokens_list.append(self.tokenizer.convert_tokens_to_ids(
                self.tokenizer.convert_text_to_tokens(masked_text)))
            segments_list.append([0] * self.tokenizer.max_seq_length)

        if masked_indices:
            transformations = []
            tokens_tensor = torch.tensor(tokens_list, device=utils.get_device())
            segments_tensor = torch.tensor(segments_list, device=utils.get_device())

            with torch.no_grad():
                preds = self.model(tokens_tensor, token_type_ids=segments_tensor)[0]

            for i in range(len(masked_indices)):
                if masked_indices[i]+1 >= self.tokenizer.max_seq_length:
                    continue
                # we do +1 b/c whole text is shifted by 1 when passed to LM
                top_preds = torch.topk(preds[i][masked_indices[i]+1], self.max_candidates)
                top_ids = top_preds.indices
                top_scores = top_preds.values
                top_tokens = self.tokenizer.convert_ids_to_tokens(top_ids)
                debug=True
                prints=[]
                for j in range(len(top_tokens)):
                    new_word = recover_word_case(top_tokens[j], text[masked_indices[i]])
                    prints.append(new_word)
                    if not new_word.isalpha():
                        continue
                    new_tokenized_text = tokenized_text.replace_word_at_index(masked_indices[i], new_word)
                    new_tokenized_text.attack_attrs["lm_score"] = top_scores[j].item()
                    transformations.append(new_tokenized_text)
                if debug:
                    print(text[masked_indices[i]], prints)
            return transformations
        else:
            return []


def recover_word_case(word, reference_word):
    """ Makes the case of `word` like the case of `reference_word`. Supports
        lowercase, UPPERCASE, and Capitalized. """
    if reference_word.islower():
        return word.lower()
    elif reference_word.isupper() and len(reference_word) > 1:
        return word.upper()
    elif reference_word[0].isupper() and reference_word[1:].islower():
        return word.capitalize()
    else:
        # if other, just do not alter the word's case
        return word
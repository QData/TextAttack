{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextAttack End-to-End\n",
    "\n",
    "This tutorial provides a broad end-to-end overview of training, evaluating, and attacking a model using TextAttack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/QData/TextAttack/blob/master/docs/2notebook/0_End_to_End.ipynb)\n",
    "\n",
    "[![View Source on GitHub](https://img.shields.io/badge/github-view%20source-black.svg)](https://github.com/QData/TextAttack/blob/master/docs/2notebook/0_End_to_End.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "First, we're going to train a model. TextAttack integrates directly with [transformers](https://github.com/huggingface/transformers/) and [datasets](https://github.com/huggingface/datasets) to train any of the `transformers` pre-trained models on datasets from `datasets`. \n",
    "\n",
    "Let's use the SNLI textual entailment dataset: it's relatively short (in word count, at least), and showcases a lot of the features of `textattack train`. Let's take a look at the dataset using `textattack peek-dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing dataset snli (/p/qdata/jy2ma/.cache/textattack/datasets/snli/plain_text/1.0.0/bb1102591c6230bd78813e229d5dd4c7fbf4fc478cec28f298761eb69e5b537c)\n",
      "\u001b[34;1mtextattack\u001b[0m: Loading \u001b[94mdatasets\u001b[0m dataset \u001b[94msnli\u001b[0m, split \u001b[94mtrain\u001b[0m.\n",
      "Loading cached shuffled indices for dataset at /p/qdata/jy2ma/.cache/textattack/datasets/snli/plain_text/1.0.0/bb1102591c6230bd78813e229d5dd4c7fbf4fc478cec28f298761eb69e5b537c/cache-27c827c079649d60.arrow\n",
      "\u001b[34;1mtextattack\u001b[0m: Number of samples: \u001b[94m550152\u001b[0m\n",
      "\u001b[34;1mtextattack\u001b[0m: Number of words per input:\n",
      "\u001b[34;1mtextattack\u001b[0m: \ttotal:   \u001b[94m11150480\u001b[0m\n",
      "\u001b[34;1mtextattack\u001b[0m: \tmean:    \u001b[94m20.27\u001b[0m\n",
      "\u001b[34;1mtextattack\u001b[0m: \tstd:     \u001b[94m6.95\u001b[0m\n",
      "\u001b[34;1mtextattack\u001b[0m: \tmin:     \u001b[94m4\u001b[0m\n",
      "\u001b[34;1mtextattack\u001b[0m: \tmax:     \u001b[94m112\u001b[0m\n",
      "\u001b[34;1mtextattack\u001b[0m: Dataset lowercased: \u001b[94mFalse\u001b[0m\n",
      "\u001b[34;1mtextattack\u001b[0m: First sample:\n",
      "Premise: A person on a horse jumps over a broken down airplane.\n",
      "Hypothesis: A person is training his horse for a competition. \n",
      "\n",
      "\u001b[34;1mtextattack\u001b[0m: Last sample:\n",
      "Premise: A man is surfing in a bodysuit in beautiful blue water.\n",
      "Hypothesis: On the beautiful blue water there is a man in a bodysuit surfing. \n",
      "\n",
      "\u001b[34;1mtextattack\u001b[0m: Found 4 distinct outputs.\n",
      "\u001b[34;1mtextattack\u001b[0m: Most common outputs:\n",
      "\t 0      (183416)\n",
      "\t 2      (183187)\n",
      "\t 1      (182764)\n",
      "\t -1     (785)\n"
     ]
    }
   ],
   "source": [
    "!textattack peek-dataset --dataset-from-huggingface snli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset looks good! It's not lowercased already, so we'll make sure our model is cased. Looks like there are some missing (-1) labels, so we need to filter those out. The longest input is 114 words, so we can cap our maximum sequence length (`--max-length`) at 128.\n",
    "\n",
    "We'll train [`distilbert-base-cased`](https://huggingface.co/transformers/model_doc/distilbert.html), since it's a relatively small model, and a good example of how we integrate with `transformers`.\n",
    "\n",
    "So we have our command:\n",
    "\n",
    "```bash\n",
    "textattack train                  \\ # Train a model with TextAttack\n",
    "    --model distilbert-base-cased \\ # Using distilbert, cased version, from `transformers`\n",
    "    --dataset snli                \\ # On the SNLI dataset\n",
    "    --max-length 128              \\ # With a maximum sequence length of 128\n",
    "    --batch-size 256              \\ # And a batch size of 256\n",
    "    --epochs 3                    \\ # For 3 epochs\n",
    "    --allowed-labels 0 1 2          # And only allow labels 0, 1, 2 (filter out -1!)\n",
    "```\n",
    "\n",
    "Now let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34;1mtextattack\u001b[0m: WARNING: TextAttack's model training feature is in beta. Please report any issues on our Github page, https://github.com/QData/TextAttack/issues.\n",
      "\u001b[34;1mtextattack\u001b[0m: Writing logs to /p/qdatatext/jy2ma/textattack/outputs/training/distilbert-base-cased-snli-2021-02-11-03-06-18-577971/log.txt.\n",
      "Reusing dataset snli (/p/qdata/jy2ma/.cache/textattack/datasets/snli/plain_text/1.0.0/bb1102591c6230bd78813e229d5dd4c7fbf4fc478cec28f298761eb69e5b537c)\n",
      "\u001b[34;1mtextattack\u001b[0m: Loading \u001b[94mdatasets\u001b[0m dataset \u001b[94msnli\u001b[0m, split \u001b[94mtrain\u001b[0m.\n",
      "Reusing dataset snli (/p/qdata/jy2ma/.cache/textattack/datasets/snli/plain_text/1.0.0/bb1102591c6230bd78813e229d5dd4c7fbf4fc478cec28f298761eb69e5b537c)\n",
      "Reusing dataset snli (/p/qdata/jy2ma/.cache/textattack/datasets/snli/plain_text/1.0.0/bb1102591c6230bd78813e229d5dd4c7fbf4fc478cec28f298761eb69e5b537c)\n",
      "Reusing dataset snli (/p/qdata/jy2ma/.cache/textattack/datasets/snli/plain_text/1.0.0/bb1102591c6230bd78813e229d5dd4c7fbf4fc478cec28f298761eb69e5b537c)\n",
      "\u001b[34;1mtextattack\u001b[0m: Loading \u001b[94mdatasets\u001b[0m dataset \u001b[94msnli\u001b[0m, split \u001b[94mvalidation\u001b[0m.\n",
      "\u001b[34;1mtextattack\u001b[0m: Loaded dataset. Found: 3 labels: [0, 1, 2]\n",
      "\u001b[34;1mtextattack\u001b[0m: Loading transformers AutoModelForSequenceClassification: distilbert-base-cased\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34;1mtextattack\u001b[0m: Training model across 1 GPUs\n",
      "\u001b[34;1mtextattack\u001b[0m: Wrote original training args to /p/qdatatext/jy2ma/textattack/outputs/training/distilbert-base-cased-snli-2021-02-11-03-06-18-577971/train_args.json.\n",
      "\u001b[34;1mtextattack\u001b[0m: ***** Running training *****\n",
      "\u001b[34;1mtextattack\u001b[0m: \tNum examples = 549367\n",
      "\u001b[34;1mtextattack\u001b[0m: \tBatch size = 128\n",
      "\u001b[34;1mtextattack\u001b[0m: \tMax sequence length = 128\n",
      "\u001b[34;1mtextattack\u001b[0m: \tNum steps = 12873\n",
      "\u001b[34;1mtextattack\u001b[0m: \tNum epochs = 3\n",
      "\u001b[34;1mtextattack\u001b[0m: \tLearning rate = 2e-05\n",
      "Loss 0.48225334872526604: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4292/4292 [44:30<00:00,  1.61it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 80.77423653040682%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 86.69985775248932%\n",
      "\u001b[34;1mtextattack\u001b[0m: Best acc found. Saved model to /p/qdatatext/jy2ma/textattack/outputs/training/distilbert-base-cased-snli-2021-02-11-03-06-18-577971/.\n",
      "\u001b[34;1mtextattack\u001b[0m: Saved updated args to /p/qdatatext/jy2ma/textattack/outputs/training/distilbert-base-cased-snli-2021-02-11-03-06-18-577971/train_args.json\n",
      "Loss 0.4194686529149018: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4292/4292 [44:36<00:00,  1.60it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 86.48826740594174%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 88.18329607803292%\n",
      "\u001b[34;1mtextattack\u001b[0m: Best acc found. Saved model to /p/qdatatext/jy2ma/textattack/outputs/training/distilbert-base-cased-snli-2021-02-11-03-06-18-577971/.\n",
      "\u001b[34;1mtextattack\u001b[0m: Saved updated args to /p/qdatatext/jy2ma/textattack/outputs/training/distilbert-base-cased-snli-2021-02-11-03-06-18-577971/train_args.json\n",
      "Loss 0.3817790571914136: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4292/4292 [44:37<00:00,  1.60it/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Train accuracy: 88.57339447036317%\n",
      "\u001b[34;1mtextattack\u001b[0m: Eval accuracy: 88.4881121723227%\n",
      "\u001b[34;1mtextattack\u001b[0m: Best acc found. Saved model to /p/qdatatext/jy2ma/textattack/outputs/training/distilbert-base-cased-snli-2021-02-11-03-06-18-577971/.\n",
      "\u001b[34;1mtextattack\u001b[0m: Saved updated args to /p/qdatatext/jy2ma/textattack/outputs/training/distilbert-base-cased-snli-2021-02-11-03-06-18-577971/train_args.json\n",
      "Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [2:14:45<00:00, 2695.00s/it]\n",
      "\u001b[34;1mtextattack\u001b[0m: Finished training. Re-loading and evaluating model from disk.\n",
      "\u001b[34;1mtextattack\u001b[0m: Loading transformers AutoModelForSequenceClassification: distilbert-base-cased\n",
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34;1mtextattack\u001b[0m: Saved model accuracy: 88.4881121723227%\n",
      "\u001b[34;1mtextattack\u001b[0m: Saved tokenizer <textattack.models.tokenizers.auto_tokenizer.AutoTokenizer object at 0x7f12d76798d0> to /p/qdatatext/jy2ma/textattack/outputs/training/distilbert-base-cased-snli-2021-02-11-03-06-18-577971/.\n",
      "\u001b[34;1mtextattack\u001b[0m: Wrote README to /p/qdatatext/jy2ma/textattack/outputs/training/distilbert-base-cased-snli-2021-02-11-03-06-18-577971/README.md.\n",
      "\u001b[34;1mtextattack\u001b[0m: Wrote final training args to /p/qdatatext/jy2ma/textattack/outputs/training/distilbert-base-cased-snli-2021-02-11-03-06-18-577971/train_args.json.\n"
     ]
    }
   ],
   "source": [
    "!textattack train --model distilbert-base-cased --dataset snli --max-length 128 --batch-size 128 --epochs 3 --allowed-labels 0 1 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We successfully fine-tuned `distilbert-base-cased` for 3 epochs. Now let's evaluate it using `textattack eval`. This is as simple as providing the path to the pretrained model to `--model`, along with the number of evaluation samples. `textattack eval` will automatically load the evaluation data from training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/p/qdata/jy2ma/miniconda3/envs/textattack-dev/bin/textattack\", line 33, in <module>\n",
      "    sys.exit(load_entry_point('textattack', 'console_scripts', 'textattack')())\n",
      "  File \"/p/qdatatext/jy2ma/textattack/TextAttack-dev/textattack/commands/textattack_cli.py\", line 42, in main\n",
      "    func.run(args)\n",
      "  File \"/p/qdatatext/jy2ma/textattack/TextAttack-dev/textattack/commands/eval_model_command.py\", line 95, in run\n",
      "    self.test_model_on_dataset(args)\n",
      "  File \"/p/qdatatext/jy2ma/textattack/TextAttack-dev/textattack/commands/eval_model_command.py\", line 39, in test_model_on_dataset\n",
      "    model = ModelArgs.create_model_from_args(args)\n",
      "  File \"/p/qdatatext/jy2ma/textattack/TextAttack-dev/textattack/model_args.py\", line 258, in create_model_from_args\n",
      "    from textattack.commands.train_model.train_args_helpers import (\n",
      "ModuleNotFoundError: No module named 'textattack.commands.train_model'\n"
     ]
    }
   ],
   "source": [
    "!textattack eval --num-examples 1000 --model /p/qdatatext/jy2ma/textattack/outputs/training/distilbert-base-cased-snli-2021-02-11-03-06-18-577971/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome -- we were able to train a model up to 86.8% validation-set accuracyâ€“ with only a single command!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack\n",
    "\n",
    "Finally, let's attack our pre-trained model. We can do this the same way as before (by providing the path to the pretrained model to `--model`). For our attack, let's use the \"TextFooler\" attack recipe, from the paper [\"Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment\" (Jin et al, 2019)](https://arxiv.org/abs/1907.11932). We can do this by passing `--recipe textfooler` to `textattack attack`.\n",
    "\n",
    "> *Warning*: We're printing out 1000 examples and, if the attack succeeds, their perturbations. The output of this command is going to be quite long!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/p/qdata/jy2ma/miniconda3/envs/textattack-dev/bin/textattack\", line 33, in <module>\n",
      "    sys.exit(load_entry_point('textattack', 'console_scripts', 'textattack')())\n",
      "  File \"/p/qdatatext/jy2ma/textattack/TextAttack-dev/textattack/commands/textattack_cli.py\", line 42, in main\n",
      "    func.run(args)\n",
      "  File \"/p/qdatatext/jy2ma/textattack/TextAttack-dev/textattack/commands/attack_command.py\", line 15, in run\n",
      "    dataset = DatasetArgs.create_dataset_from_args(attack_args)\n",
      "  File \"/p/qdatatext/jy2ma/textattack/TextAttack-dev/textattack/dataset_args.py\", line 274, in create_dataset_from_args\n",
      "    raise ValueError(\"Must supply pretrained model or dataset\")\n",
      "ValueError: Must supply pretrained model or dataset\n"
     ]
    }
   ],
   "source": [
    "!textattack attack --recipe textfooler --num-examples 1000 --model /p/qdatatext/jy2ma/textattack/outputs/training/distilbert-base-cased-snli-2021-02-11-03-06-18-577971/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our model was 86.8% successful (makes sense - same evaluation set as `textattack eval`!), meaning that TextAttack attacked the model with 868 examples (since the attack won't run if an example is originally mispredicted). The attack success rate was 88.7%, meaning that TextFooler failed to find an adversarial example only 11.3% of the time.\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "That's all, folks! We've learned how to train, evaluate, and attack a model with TextAttack, using only three commands! ðŸ˜€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

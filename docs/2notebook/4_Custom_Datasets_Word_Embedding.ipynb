{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextAttack with Custom Dataset and Word Embedding. This tutorial will show you how to use textattack with any dataset and word embedding you may want to use",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/QData/TextAttack/blob/master/docs/2notebook/4_Custom_Datasets_Word_Embedding.ipynb)\n",
    "\n",
    "[![View Source on GitHub](https://img.shields.io/badge/github-view%20source-black.svg)](https://github.com/QData/TextAttack/blob/master/docs/2notebook/4_Custom_Datasets_Word_Embedding.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GjBxEfZGpGTf"
   },
   "source": [
    "**Installing TextAttack**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iZiA1ywdtiDj",
    "outputId": "ffc58d2b-c37a-4578-817d-6a254b3acd32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textattack\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/82/2f16ef7f22f19b3a49bbcd079dc31e53d362e1ef1299298c3eda05cf2b3a/textattack-0.2.15-py3-none-any.whl (349kB)\n",
      "\r",
      "\u001b[K     |█                               | 10kB 16.6MB/s eta 0:00:01\r",
      "\u001b[K     |█▉                              | 20kB 22.2MB/s eta 0:00:01\r",
      "\u001b[K     |██▉                             | 30kB 11.8MB/s eta 0:00:01\r",
      "\u001b[K     |███▊                            | 40kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |████▊                           | 51kB 7.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████▋                          | 61kB 7.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████▋                         | 71kB 7.6MB/s eta 0:00:01\r",
      "\u001b[K     |███████▌                        | 81kB 7.6MB/s eta 0:00:01\r",
      "\u001b[K     |████████▍                       | 92kB 7.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▍                      | 102kB 8.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▎                     | 112kB 8.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▎                    | 122kB 8.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▏                   | 133kB 8.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▏                  | 143kB 8.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 153kB 8.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 163kB 8.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████                | 174kB 8.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▉               | 184kB 8.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▉              | 194kB 8.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▊             | 204kB 8.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▊            | 215kB 8.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▋           | 225kB 8.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▋          | 235kB 8.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▌         | 245kB 8.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▌        | 256kB 8.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▍       | 266kB 8.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▎      | 276kB 8.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▎     | 286kB 8.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▏    | 296kB 8.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▏   | 307kB 8.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 317kB 8.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 327kB 8.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 337kB 8.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 348kB 8.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 358kB 8.2MB/s \n",
      "\u001b[?25hCollecting word2number\n",
      "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from textattack) (8.7.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from textattack) (3.2.5)\n",
      "Requirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from textattack) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from textattack) (3.0.12)\n",
      "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from textattack) (1.4.1)\n",
      "Collecting datasets\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/90/43b396481a8298c6010afb93b3c1e71d4ba6f8c10797a7da8eb005e45081/datasets-1.5.0-py3-none-any.whl (192kB)\n",
      "\u001b[K     |████████████████████████████████| 194kB 11.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from textattack) (1.7.1)\n",
      "Collecting numpy<1.19.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/c6/58e517e8b1fb192725cfa23c01c2e60e4e6699314ee9684a1c5f5c9b27e1/numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1MB)\n",
      "\u001b[K     |████████████████████████████████| 20.1MB 1.3MB/s \n",
      "\u001b[?25hCollecting lemminflect\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/67/d04ca98b661d4ad52b9b965c9dabb1f1a2c85541d20f8decb9a9df4e4b32/lemminflect-0.2.2-py3-none-any.whl (769kB)\n",
      "\u001b[K     |████████████████████████████████| 778kB 42.0MB/s \n",
      "\u001b[?25hCollecting language-tool-python\n",
      "  Downloading https://files.pythonhosted.org/packages/37/26/48b22ad565fd372edec3577218fb817e0e6626bf4e658033197470ad92b3/language_tool_python-2.5.3-py3-none-any.whl\n",
      "Collecting transformers>=3.3.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0MB 31.6MB/s \n",
      "\u001b[?25hCollecting bert-score>=0.3.5\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/27/ccf86d5dfc19f89bee4449e96ac6e0f7c312f1614de86609c5f6da5c40af/bert_score-0.3.8-py3-none-any.whl (58kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 7.9MB/s \n",
      "\u001b[?25hCollecting lru-dict\n",
      "  Downloading https://files.pythonhosted.org/packages/68/ea/997af58d4e6da019ad825a412f93081d9df67e9dda11cfb026a3d7cd0b6c/lru-dict-1.1.7.tar.gz\n",
      "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from textattack) (1.1.5)\n",
      "Collecting flair==0.6.1.post1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/49/a812ed93088ba9519cbb40eb9f52341694b31cfa126bfddcd9db3761f3ac/flair-0.6.1.post1-py3-none-any.whl (337kB)\n",
      "\u001b[K     |████████████████████████████████| 337kB 41.4MB/s \n",
      "\u001b[?25hCollecting terminaltables\n",
      "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
      "Collecting num2words\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/eb/a2/ea800689730732e27711c41beed4b2a129b34974435bdc450377ec407738/num2words-0.5.10-py3-none-any.whl (101kB)\n",
      "\u001b[K     |████████████████████████████████| 102kB 10.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from textattack) (1.8.0+cu101)\n",
      "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from textattack) (4.41.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->textattack) (1.15.0)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets->textattack) (3.0.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets->textattack) (0.70.11.1)\n",
      "Collecting huggingface-hub<0.1.0\n",
      "  Downloading https://files.pythonhosted.org/packages/af/07/bf95f398e6598202d878332280f36e589512174882536eb20d792532a57d/huggingface_hub-0.0.7-py3-none-any.whl\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets->textattack) (0.3.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets->textattack) (2.23.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets->textattack) (3.7.2)\n",
      "Collecting xxhash\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/27/1c0b37c53a7852f1c190ba5039404d27b3ae96a55f48203a74259f8213c9/xxhash-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
      "\u001b[K     |████████████████████████████████| 245kB 46.3MB/s \n",
      "\u001b[?25hCollecting fsspec\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/0d/a6bfee0ddf47b254286b9bd574e6f50978c69897647ae15b14230711806e/fsspec-0.8.7-py3-none-any.whl (103kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 47.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=3.3.0->textattack) (20.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.3.0->textattack) (2019.12.20)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2MB 38.1MB/s \n",
      "\u001b[?25hCollecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 46.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from bert-score>=0.3.5->textattack) (3.2.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->textattack) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->textattack) (2.8.1)\n",
      "Collecting janome\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/63/98858cbead27df7536c7e300c169da0999e9704d02220dc6700b804eeff0/Janome-0.4.1-py2.py3-none-any.whl (19.7MB)\n",
      "\u001b[K     |████████████████████████████████| 19.7MB 229kB/s \n",
      "\u001b[?25hCollecting bpemb>=0.3.2\n",
      "  Downloading https://files.pythonhosted.org/packages/91/77/3f0f53856e86af32b1d3c86652815277f7b5f880002584eb30db115b6df5/bpemb-0.3.2-py3-none-any.whl\n",
      "Collecting sqlitedict>=1.6.0\n",
      "  Downloading https://files.pythonhosted.org/packages/5c/2d/b1d99e9ad157dd7de9cd0d36a8a5876b13b55e4b75f7498bc96035fb4e96/sqlitedict-1.7.0.tar.gz\n",
      "Collecting ftfy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/06/e5c80e2e0f979628d47345efba51f7ba386fe95963b11c594209085f5a9b/ftfy-5.9.tar.gz (66kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 8.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from flair==0.6.1.post1->textattack) (0.1.2)\n",
      "Collecting sentencepiece!=0.1.92\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 38.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair==0.6.1.post1->textattack) (0.22.2.post1)\n",
      "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (from flair==0.6.1.post1->textattack) (3.6.4)\n",
      "Collecting langdetect\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a3/8407c1e62d5980188b4acc45ef3d94b933d14a2ebc9ef3505f22cf772570/langdetect-1.0.8.tar.gz (981kB)\n",
      "\u001b[K     |████████████████████████████████| 983kB 38.4MB/s \n",
      "\u001b[?25hCollecting segtok>=1.5.7\n",
      "  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair==0.6.1.post1->textattack) (4.2.6)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair==0.6.1.post1->textattack) (0.8.9)\n",
      "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair==0.6.1.post1->textattack) (3.6.0)\n",
      "Collecting konoha<5.0.0,>=4.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/02/be/4dd30d56a0a19619deb9bf41ba8202709fa83b1b301b876572cd6dc38117/konoha-4.6.4-py3-none-any.whl\n",
      "Collecting deprecated>=1.2.4\n",
      "  Downloading https://files.pythonhosted.org/packages/fb/73/994edfcba74443146c84b91921fcc269374354118d4f452fb0c54c1cbb12/Deprecated-1.2.12-py2.py3-none-any.whl\n",
      "Collecting mpld3==0.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
      "\u001b[K     |████████████████████████████████| 798kB 36.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from num2words->textattack) (0.6.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->textattack) (3.7.4.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets->textattack) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets->textattack) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets->textattack) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets->textattack) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets->textattack) (3.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers>=3.3.0->textattack) (2.4.7)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.3.0->textattack) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.3.0->textattack) (1.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (0.10.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->flair==0.6.1.post1->textattack) (0.2.5)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair==0.6.1.post1->textattack) (2.5)\n",
      "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair==0.6.1.post1->textattack) (3.11.3)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair==0.6.1.post1->textattack) (0.16.0)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->flair==0.6.1.post1->textattack) (4.2.0)\n",
      "Collecting overrides<4.0.0,>=3.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair==0.6.1.post1->textattack) (1.12.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx->hyperopt>=0.1.1->flair==0.6.1.post1->textattack) (4.4.2)\n",
      "Building wheels for collected packages: word2number, lru-dict, terminaltables, sacremoses, sqlitedict, ftfy, langdetect, segtok, mpld3, overrides\n",
      "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for word2number: filename=word2number-1.1-cp37-none-any.whl size=5589 sha256=427750cc3e660a9b3f02aefb6e4b78a66f5e1bfcd35af361b563dfc848be98d8\n",
      "  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
      "  Building wheel for lru-dict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for lru-dict: filename=lru_dict-1.1.7-cp37-cp37m-linux_x86_64.whl size=28374 sha256=8c67d92d00fabc3c19298ae47142ae0e9f1d438ed37ae3dbab7056eaacac9ffe\n",
      "  Stored in directory: /root/.cache/pip/wheels/ae/51/23/0a416781dead9225c7d66d25b9f223c7e32304e99a0b01d566\n",
      "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp37-none-any.whl size=15356 sha256=191cbff164189dd64d1ae2bb05d586d597610eaaa3f35a0a174ebe9c3af47f20\n",
      "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=0db5465a650605c29d82645df13f051157dad9be13c77d64916965955426447b\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-cp37-none-any.whl size=14376 sha256=294bf8660c538b3d7c47ffcc9bc5a5ad0a1cb763e842764d30daf1e691370b8c\n",
      "  Stored in directory: /root/.cache/pip/wheels/cf/c6/4f/2c64a43f041415eb8b8740bd80e15e92f0d46c5e464d8e4b9b\n",
      "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for ftfy: filename=ftfy-5.9-cp37-none-any.whl size=46451 sha256=1ce10ba392b28688908b5219ae089e1d93a8a9a486150b13a0f043482bbe52f6\n",
      "  Stored in directory: /root/.cache/pip/wheels/5e/2e/f0/b07196e8c929114998f0316894a61c752b63bfa3fdd50d2fc3\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.8-cp37-none-any.whl size=993193 sha256=d4cff043f0fd9504cb71c07fd9e82a5242198fa545d0e8e45f589b031404771c\n",
      "  Stored in directory: /root/.cache/pip/wheels/8d/b3/aa/6d99de9f3841d7d3d40a60ea06e6d669e8e5012e6c8b947a57\n",
      "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for segtok: filename=segtok-1.5.10-cp37-none-any.whl size=25019 sha256=79ba0c731c174d8d1fa0a8a442ec1b9c5d4adf5e112de3a9663bd6b6c24f3c6d\n",
      "  Stored in directory: /root/.cache/pip/wheels/b4/39/f6/9ca1c5cabde964d728023b5751c3a206a5c8cc40252321fb6b\n",
      "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for mpld3: filename=mpld3-0.3-cp37-none-any.whl size=116679 sha256=93e9d67b96b56cdbd0b0523e6c9a247acff2eea5fc15a19b97660287a1f9a29f\n",
      "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
      "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for overrides: filename=overrides-3.1.0-cp37-none-any.whl size=10174 sha256=c57ac4acfbca147f0b897a8273ee544dcbc1e867828d80a046886bd6bc078b8a\n",
      "  Stored in directory: /root/.cache/pip/wheels/5c/24/13/6ef8600e6f147c95e595f1289a86a3cc82ed65df57582c65a9\n",
      "Successfully built word2number lru-dict terminaltables sacremoses sqlitedict ftfy langdetect segtok mpld3 overrides\n",
      "\u001b[31mERROR: tensorflow 2.4.1 has requirement numpy~=1.19.2, but you'll have numpy 1.18.5 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: konoha 4.6.4 has requirement requests<3.0.0,>=2.25.1, but you'll have requests 2.23.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: word2number, huggingface-hub, numpy, xxhash, fsspec, datasets, lemminflect, language-tool-python, tokenizers, sacremoses, transformers, bert-score, lru-dict, janome, sentencepiece, bpemb, sqlitedict, ftfy, langdetect, segtok, overrides, konoha, deprecated, mpld3, flair, terminaltables, num2words, textattack\n",
      "  Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "Successfully installed bert-score-0.3.8 bpemb-0.3.2 datasets-1.5.0 deprecated-1.2.12 flair-0.6.1.post1 fsspec-0.8.7 ftfy-5.9 huggingface-hub-0.0.7 janome-0.4.1 konoha-4.6.4 langdetect-1.0.8 language-tool-python-2.5.3 lemminflect-0.2.2 lru-dict-1.1.7 mpld3-0.3 num2words-0.5.10 numpy-1.18.5 overrides-3.1.0 sacremoses-0.0.43 segtok-1.5.10 sentencepiece-0.1.95 sqlitedict-1.7.0 terminaltables-3.1.0 textattack-0.2.15 tokenizers-0.10.1 transformers-4.4.2 word2number-1.1 xxhash-2.0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "numpy"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install textattack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WVki6Bvbjur"
   },
   "source": [
    "**Importing the Model**\n",
    "\n",
    "We start by choosing a pretrained model we want to attack. In this example we will use the albert base v2 model from HuggingFace. This model was trained with data from imbd, a set of movie reviews with either positive or negative labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585,
     "referenced_widgets": [
      "1905ff29aaa242a88dc93f3247065364",
      "917713cc9b1344c7a7801144f04252bc",
      "b65d55c5b9f445a6bfd585f6237d22ca",
      "38b56a89b2ae4a8ca93c03182db26983",
      "26082a081d1c49bd907043a925cf88df",
      "1c3edce071ad4a2a99bf3e34ea40242c",
      "f9c265a003444a03bde78e18ed3f5a7e",
      "3cb9eb594c8640ffbfd4a0b1139d571a",
      "7d29511ba83a4eaeb4a2e5cd89ca1990",
      "136f44f7b8fa433ebff6d0a534c0588b",
      "2658e486ee77468a99ab4edc7b5191d8",
      "39bfd8c439b847e4bdfeee6e66ae86f3",
      "7ca4ce3d902d42758eb1fc02b9b211d3",
      "222cacceca11402db10ff88a92a2d31d",
      "108d2b83dff244edbebf4f8909dce789",
      "c06317aaf0064cb9b6d86d032821a8e2",
      "c18ac12f8c6148b9aa2d69885351fbcb",
      "b11ad31ee69441df8f0447a4ae62ce75",
      "a7e846fdbda740a38644e28e11a67707",
      "b38d5158e5584461bfe0b2f8ed3b0dc2",
      "3bdef9b4157e41f3a01f25b07e8efa48",
      "69e19afa8e2c49fbab0e910a5929200f",
      "2627a092f0c041c0a5f67451b1bd8b2b",
      "1780cb5670714c0a9b7a94b92ffc1819",
      "1ac87e683d2e4951ac94e25e8fe88d69",
      "02daee23726349a69d4473814ede81c3",
      "1fac551ad9d840f38b540ea5c364af70",
      "1027e6f245924195a930aca8c3844f44",
      "5b863870023e4c438ed75d830c13c5ac",
      "9ec55c6e2c4e40daa284596372728213",
      "5e2d17ed769d496db38d053cc69a914c",
      "dedaafae3bcc47f59b7d9b025b31fd0c",
      "8c2f5cda0ae9472fa7ec2b864d0bdc0e",
      "2a35d22dd2604950bae55c7c51f4af2c",
      "4c23ca1540fd48b1ac90d9365c9c6427",
      "3e4881a27c36472ab4c24167da6817cf",
      "af32025d22534f9da9e769b02f5e6422",
      "7af34c47299f458789e03987026c3519",
      "ed0ab8c7456a42618d6cbf6fd496b7b3",
      "25fc5fdac77247f9b029ada61af630fd"
     ]
    },
    "id": "4ZEnCFoYv-y7",
    "outputId": "c6c57cb9-6d6e-4efd-988f-c794356d4719"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34;1mtextattack\u001b[0m: Updating TextAttack package dependencies.\n",
      "\u001b[34;1mtextattack\u001b[0m: Downloading NLTK required packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package omw to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/omw.zip.\n",
      "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34;1mtextattack\u001b[0m: Downloading https://textattack.s3.amazonaws.com/word_embeddings/paragramcf.\n",
      "100%|██████████| 481M/481M [00:12<00:00, 38.2MB/s]\n",
      "\u001b[34;1mtextattack\u001b[0m: Unzipping file /root/.cache/textattack/tmpl7t854zd.zip to /root/.cache/textattack/word_embeddings/paragramcf.\n",
      "\u001b[34;1mtextattack\u001b[0m: Successfully saved word_embeddings/paragramcf to cache.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1905ff29aaa242a88dc93f3247065364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=727.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d29511ba83a4eaeb4a2e5cd89ca1990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=46747112.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c18ac12f8c6148b9aa2d69885351fbcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=760289.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac87e683d2e4951ac94e25e8fe88d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=156.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c2f5cda0ae9472fa7ec2b864d0bdc0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=25.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from textattack.models.tokenizers import AutoTokenizer\n",
    "from textattack.models.wrappers import HuggingFaceModelWrapper\n",
    "\n",
    "# #https://huggingface.co/textattack\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\"textattack/albert-base-v2-imdb\")\n",
    "tokenizer = AutoTokenizer(\"textattack/albert-base-v2-imdb\")\n",
    "#We wrap the model so it can be used by textattack\n",
    "model_wrapper = HuggingFaceModelWrapper(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D61VLa8FexyK"
   },
   "source": [
    "**Creating A Custom Dataset**\n",
    "\n",
    "Textattack takes in dataset in the form of a list of tuples. The tuple can be in the form of (\"string\", label) or (\"string\", label, label). In this case we will use former one, since we want to create a custom movie review dataset with label 0 representing a positive review, and label 1 representing a negative review.\n",
    "\n",
    "For simplicity, I created a dataset consisting of 4 reviews, the 1st and 4th review have \"correct\" labels, while the 2nd and 3rd review have \"incorrect\" labels. We will see how this impacts perturbation later in this tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nk_MUu5Duf1V"
   },
   "outputs": [],
   "source": [
    "# dataset: An iterable of (text, ground_truth_output) pairs.\n",
    "#0 means the review is negative\n",
    "#1 means the review is positive\n",
    "custom_dataset = [\n",
    "    ('I hate this movie', 0), #A negative comment, with a negative label\n",
    "    ('I hate this movie', 1), #A negative comment, with a positive label\n",
    "    ('I love this movie', 0), #A positive comment, with a negative label\n",
    "    ('I love this movie', 1), #A positive comment, with a positive label\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijVmi6PbiUYZ"
   },
   "source": [
    "**Creating An Attack**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-iEH_hf6iMEw",
    "outputId": "0c836c5b-ddd5-414d-f73d-da04067054d8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34;1mtextattack\u001b[0m: Unknown if model of class <class 'transformers.models.albert.modeling_albert.AlbertForSequenceClassification'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    }
   ],
   "source": [
    "from textattack.shared import Attack\n",
    "from textattack.search_methods import GreedySearch\n",
    "from textattack.constraints.pre_transformation import RepeatModification, StopwordModification\n",
    "from textattack.goal_functions import UntargetedClassification\n",
    "from textattack.transformations import WordSwapEmbedding\n",
    "from textattack.constraints.pre_transformation import RepeatModification\n",
    "from textattack.constraints.pre_transformation import StopwordModification\n",
    "\n",
    "# We'll use untargeted classification as the goal function.\n",
    "goal_function = UntargetedClassification(model_wrapper)\n",
    "# We'll to use our WordSwapEmbedding as the attack transformation.\n",
    "transformation = WordSwapEmbedding() \n",
    "# We'll constrain modification of already modified indices and stopwords\n",
    "constraints = [RepeatModification(),\n",
    "               StopwordModification()]\n",
    "# We'll use the Greedy search method\n",
    "search_method = GreedySearch()\n",
    "# Now, let's make the attack from the 4 components:\n",
    "attack = Attack(goal_function, constraints, transformation, search_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hUA8ntnfJzH"
   },
   "source": [
    "**Attack Results With Custom Dataset**\n",
    "\n",
    "As you can see, the attack fools the model by changing a few words in the 1st and 4th review.\n",
    "\n",
    "The attack skipped the 2nd and and 3rd review because since it they were labeled incorrectly, they managed to fool the model without any modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ivoHEOXfIfN",
    "outputId": "9ec660b6-44fc-4354-9dd1-1641b6f4c986"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m0 (99%)\u001b[0m --> \u001b[92m1 (81%)\u001b[0m\n",
      "\n",
      "\u001b[91mI\u001b[0m \u001b[91mhate\u001b[0m this \u001b[91mmovie\u001b[0m\n",
      "\n",
      "\u001b[92mdid\u001b[0m \u001b[92mhateful\u001b[0m this \u001b[92mfootage\u001b[0m\n",
      "\u001b[91m0 (99%)\u001b[0m --> \u001b[37m[SKIPPED]\u001b[0m\n",
      "\n",
      "I hate this movie\n",
      "\u001b[92m1 (96%)\u001b[0m --> \u001b[37m[SKIPPED]\u001b[0m\n",
      "\n",
      "I love this movie\n",
      "\u001b[92m1 (96%)\u001b[0m --> \u001b[91m0 (99%)\u001b[0m\n",
      "\n",
      "I \u001b[92mlove\u001b[0m this movie\n",
      "\n",
      "I \u001b[91miove\u001b[0m this movie\n"
     ]
    }
   ],
   "source": [
    "results_iterable = attack.attack_dataset(custom_dataset)\n",
    "\n",
    "for result in results_iterable:\n",
    "  print(result.__str__(color_method='ansi'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "foFZmk8vY5z0"
   },
   "source": [
    "**Creating A Custom Word Embedding**\n",
    "\n",
    "In textattack, a pre-trained word embedding is necessary in transformation in order to find synonym replacements, and in constraints to check the semantic validity of the transformation. To use custom pre-trained word embeddings, you can either create a new class that inherits the AbstractWordEmbedding class, or use the WordEmbedding class which takes in 4 parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "owj_jMHRxEF5"
   },
   "outputs": [],
   "source": [
    "from textattack.shared import WordEmbedding\n",
    "\n",
    "embedding_matrix = [[1.0], [2.0], [3.0], [4.0]] #2-D array of shape N x D where N represents size of vocab and D is the dimension of embedding vectors.\n",
    "word2index = {\"hate\":0, \"despise\":1, \"like\":2, \"love\":3} #dictionary that maps word to its index with in the embedding matrix.\n",
    "index2word = {0:\"hate\", 1: \"despise\", 2:\"like\", 3:\"love\"} #dictionary that maps index to its word.\n",
    "nn_matrix = [[0, 1, 2, 3], [1, 0, 2, 3], [2, 1, 3, 0], [3, 2, 1, 0]] #2-D integer array of shape N x K where N represents size of vocab and K is the top-K nearest neighbours.\n",
    "\n",
    "embedding = WordEmbedding(embedding_matrix, word2index, index2word, nn_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9ZEV_ykhmBn"
   },
   "source": [
    "**Attack Results With Custom Dataset and Word Embedding**\n",
    "\n",
    "Now if we run the attack again with the custom word embedding, you will notice the modifications are limited to the vocab provided by our custom word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gZ98UZ6I5sIn",
    "outputId": "59a653cb-85cb-46b5-d81b-c1a05ebe8a3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91m0 (99%)\u001b[0m --> \u001b[92m1 (98%)\u001b[0m\n",
      "\n",
      "I \u001b[91mhate\u001b[0m this movie\n",
      "\n",
      "I \u001b[92mlike\u001b[0m this movie\n",
      "\u001b[91m0 (99%)\u001b[0m --> \u001b[37m[SKIPPED]\u001b[0m\n",
      "\n",
      "I hate this movie\n",
      "\u001b[92m1 (96%)\u001b[0m --> \u001b[37m[SKIPPED]\u001b[0m\n",
      "\n",
      "I love this movie\n",
      "\u001b[92m1 (96%)\u001b[0m --> \u001b[91m0 (99%)\u001b[0m\n",
      "\n",
      "I \u001b[92mlove\u001b[0m this movie\n",
      "\n",
      "I \u001b[91mdespise\u001b[0m this movie\n"
     ]
    }
   ],
   "source": [
    "from textattack.attack_results import SuccessfulAttackResult\n",
    "\n",
    "transformation = WordSwapEmbedding(3, embedding) \n",
    "\n",
    "attack = Attack(goal_function, constraints, transformation, search_method)\n",
    "\n",
    "results_iterable = attack.attack_dataset(custom_dataset)\n",
    "\n",
    "for result in results_iterable:\n",
    "  print(result.__str__(color_method='ansi'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Custom Data and Embedding with TextAttack.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
